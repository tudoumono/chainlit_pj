#!/usr/bin/env python3
"""
åŒ…æ‹¬çš„ãƒ†ã‚¹ãƒˆå®Ÿè¡Œã‚¹ã‚¯ãƒªãƒ—ãƒˆ
å…¨ãƒ†ã‚¹ãƒˆã‚«ãƒ†ã‚´ãƒªã‚’é †æ¬¡å®Ÿè¡Œã—ã€çµæœã‚’ãƒ¬ãƒãƒ¼ãƒˆ
"""

import json
import os
import time
import subprocess
from datetime import datetime
from comprehensive_test_cases import ComprehensiveTestSuite
from ui_checker import ChainlitUIChecker


class TestExecutor:
    def __init__(self):
        self.project_root = "/root/mywork/chainlit_pj"
        self.results_dir = f"{self.project_root}/test_results"
        os.makedirs(self.results_dir, exist_ok=True)
        
        # ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆã‚’èª­ã¿è¾¼ã¿
        self.test_suite = ComprehensiveTestSuite()
        self.all_tests = self.test_suite.generate_comprehensive_test_suite()
        
        # ãƒ†ã‚¹ãƒˆå®Ÿè¡ŒçŠ¶æ³
        self.execution_results = {
            "start_time": datetime.now().isoformat(),
            "test_results": {},
            "summary": {
                "total": 0,
                "passed": 0,
                "failed": 0,
                "skipped": 0
            },
            "performance_metrics": {}
        }
    
    def check_prerequisites(self):
        """ãƒ†ã‚¹ãƒˆå®Ÿè¡Œã®å‰ææ¡ä»¶ãƒã‚§ãƒƒã‚¯"""
        print("ğŸ” å‰ææ¡ä»¶ã‚’ãƒã‚§ãƒƒã‚¯ä¸­...")
        
        prerequisites = {
            "chainlit_app_running": False,
            "dependencies_installed": False,
            "webdriver_available": False
        }
        
        # Chainlitã‚¢ãƒ—ãƒªã®èµ·å‹•ç¢ºèª
        try:
            import requests
            response = requests.get("http://localhost:8000", timeout=5)
            prerequisites["chainlit_app_running"] = response.status_code == 200
        except:
            prerequisites["chainlit_app_running"] = False
        
        # ä¾å­˜é–¢ä¿‚ã®ç¢ºèª
        try:
            import selenium
            import requests
            prerequisites["dependencies_installed"] = True
        except ImportError:
            prerequisites["dependencies_installed"] = False
        
        # WebDriverã®ç¢ºèª
        try:
            checker = ChainlitUIChecker()
            checker.setup_driver()
            checker.driver.quit()
            prerequisites["webdriver_available"] = True
        except:
            prerequisites["webdriver_available"] = False
        
        # çµæœã®è¡¨ç¤º
        for check, status in prerequisites.items():
            status_icon = "âœ…" if status else "âŒ"
            print(f"  {status_icon} {check}: {'OK' if status else 'NG'}")
        
        all_ok = all(prerequisites.values())
        
        if not all_ok:
            print("\nâŒ å‰ææ¡ä»¶ã‚’æº€ãŸã—ã¦ã„ã¾ã›ã‚“ã€‚ä»¥ä¸‹ã‚’ç¢ºèªã—ã¦ãã ã•ã„ï¼š")
            if not prerequisites["chainlit_app_running"]:
                print("  - Chainlitã‚¢ãƒ—ãƒªã‚’èµ·å‹•: chainlit run app.py --host 0.0.0.0 --port 8000")
            if not prerequisites["dependencies_installed"]:
                print("  - ä¾å­˜é–¢ä¿‚ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«: pip install selenium requests")
            if not prerequisites["webdriver_available"]:
                print("  - Chrome WebDriverã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«: apt-get install chromium-browser")
            return False
        
        print("âœ… å…¨ã¦ã®å‰ææ¡ä»¶ã‚’æº€ãŸã—ã¦ã„ã¾ã™\n")
        return True
    
    def execute_ui_tests(self):
        """UIé–¢é€£ãƒ†ã‚¹ãƒˆã®å®Ÿè¡Œ"""
        print("ğŸ–¼ï¸ UI ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œä¸­...")
        
        try:
            checker = ChainlitUIChecker()
            
            # åŸºæœ¬UIãƒ†ã‚¹ãƒˆ
            ui_results = checker.test_ui_elements()
            self.execution_results["test_results"]["UI_RENDERING"] = {
                "status": "PASSED" if ui_results.get("chat_input_exists", False) else "FAILED",
                "details": ui_results,
                "screenshots": [v for k, v in ui_results.items() if "screenshot" in k or k.endswith(".png")]
            }
            
            # ãƒãƒ£ãƒƒãƒˆæ©Ÿèƒ½ãƒ†ã‚¹ãƒˆ
            chat_results = checker.test_chat_functionality()
            self.execution_results["test_results"]["CHAT_FUNCTIONALITY"] = {
                "status": "PASSED" if chat_results.get("chat_test_success", False) else "FAILED", 
                "details": chat_results,
                "screenshots": [v for k, v in chat_results.items() if k.endswith(".png")]
            }
            
            # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æƒ…å ±ã‚’è¨˜éŒ²
            if "load_time" in ui_results:
                self.execution_results["performance_metrics"]["page_load_time"] = ui_results["load_time"]
            
        except Exception as e:
            print(f"âŒ UIãƒ†ã‚¹ãƒˆå®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
            self.execution_results["test_results"]["UI_ERROR"] = {
                "status": "ERROR",
                "error": str(e)
            }
    
    def execute_manual_tests(self):
        """æ‰‹å‹•ãƒ†ã‚¹ãƒˆé …ç›®ã®ä¸€è¦§ç”Ÿæˆ"""
        print("ğŸ“‹ æ‰‹å‹•ãƒ†ã‚¹ãƒˆé …ç›®ã‚’ç”Ÿæˆä¸­...")
        
        manual_categories = ["AI_INTEGRATION", "TOOLS_INTEGRATION", "DATA_PERSISTENCE", 
                           "ERROR_HANDLING", "SECURITY"]
        
        manual_tests = {}
        
        for category in manual_categories:
            if category in self.all_tests["test_cases"]:
                manual_tests[category] = []
                for test in self.all_tests["test_cases"][category]:
                    manual_tests[category].append({
                        "id": test["id"],
                        "name": test["name"],
                        "description": test["description"],
                        "priority": test.get("priority", "MEDIUM"),
                        "steps": test["steps"],
                        "expected_results": test["expected_results"],
                        "status": "PENDING"  # æ‰‹å‹•å®Ÿè¡Œå¾…ã¡
                    })
        
        self.execution_results["manual_tests"] = manual_tests
    
    def generate_test_checklist(self):
        """å®Ÿè¡Œå¯èƒ½ãªãƒ†ã‚¹ãƒˆãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆã‚’ç”Ÿæˆ"""
        print("ğŸ“ å®Ÿè¡Œç”¨ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆã‚’ç”Ÿæˆä¸­...")
        
        checklist = {
            "metadata": {
                "generated": datetime.now().isoformat(),
                "total_tests": self.all_tests["statistics"]["total_tests"]
            },
            "automated_tests": {
                "ui_rendering": "è‡ªå‹•å®Ÿè¡Œæ¸ˆã¿",
                "user_interaction": "è‡ªå‹•å®Ÿè¡Œæ¸ˆã¿", 
                "chat_functionality": "è‡ªå‹•å®Ÿè¡Œæ¸ˆã¿",
                "performance": "è‡ªå‹•æ¸¬å®šæ¸ˆã¿"
            },
            "manual_tests": {}
        }
        
        # æ‰‹å‹•ãƒ†ã‚¹ãƒˆç”¨ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ
        priority_order = ["HIGH", "MEDIUM", "LOW"]
        
        for priority in priority_order:
            checklist["manual_tests"][priority] = []
            
            for category, tests in self.all_tests["test_cases"].items():
                category_name = self.all_tests["categories"][category]
                
                for test in tests:
                    if test.get("priority") == priority:
                        checklist["manual_tests"][priority].append({
                            "category": category_name,
                            "id": test["id"],
                            "name": test["name"],
                            "description": test["description"],
                            "estimated_time": self._estimate_test_time(test),
                            "requires_setup": self._check_setup_requirements(test),
                            "completed": False
                        })
        
        # ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜
        checklist_file = os.path.join(self.results_dir, "manual_test_checklist.json")
        with open(checklist_file, "w", encoding="utf-8") as f:
            json.dump(checklist, f, indent=2, ensure_ascii=False)
        
        return checklist_file
    
    def _estimate_test_time(self, test):
        """ãƒ†ã‚¹ãƒˆå®Ÿè¡Œæ™‚é–“ã®æ¨å®š"""
        step_count = len(test.get("steps", []))
        complexity_factors = {
            "CHAT_FUNCTIONALITY": 2,
            "AI_INTEGRATION": 3,
            "TOOLS_INTEGRATION": 3,
            "DATA_PERSISTENCE": 2,
            "ERROR_HANDLING": 2,
            "SECURITY": 4,
            "PERFORMANCE": 3
        }
        
        base_time = step_count * 30  # 1ã‚¹ãƒ†ãƒƒãƒ—30ç§’ã®åŸºæº–
        complexity = complexity_factors.get(test.get("category", ""), 1)
        
        estimated_seconds = base_time * complexity
        return f"{estimated_seconds // 60}åˆ†{estimated_seconds % 60}ç§’"
    
    def _check_setup_requirements(self, test):
        """ãƒ†ã‚¹ãƒˆå®Ÿè¡Œã«å¿…è¦ãªè¨­å®šã®ç¢ºèª"""
        requirements = []
        
        # ã‚«ãƒ†ã‚´ãƒªåˆ¥ã®è¦æ±‚äº‹é …
        if test.get("category") == "TOOLS_INTEGRATION":
            requirements.append("APIã‚­ãƒ¼è¨­å®š")
            requirements.append("ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢è¨­å®š")
        elif test.get("category") == "ERROR_HANDLING":
            requirements.append("ãƒ†ã‚¹ãƒˆç”¨ç„¡åŠ¹APIã‚­ãƒ¼")
        elif test.get("category") == "SECURITY":
            requirements.append("é–‹ç™ºè€…ãƒ„ãƒ¼ãƒ«")
        elif test.get("category") == "PERFORMANCE":
            requirements.append("ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¸¬å®šãƒ„ãƒ¼ãƒ«")
        
        # ãƒ†ã‚¹ãƒˆåã‹ã‚‰æ¨æ¸¬
        test_name = test.get("name", "").lower()
        if "webæ¤œç´¢" in test_name:
            requirements.append("ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆæ¥ç¶š")
        if "ãƒšãƒ«ã‚½ãƒŠ" in test_name:
            requirements.append("ãƒšãƒ«ã‚½ãƒŠä½œæˆæ¨©é™")
        
        return requirements
    
    def generate_final_report(self):
        """æœ€çµ‚ãƒ†ã‚¹ãƒˆãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆ"""
        print("ğŸ“Š æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆä¸­...")
        
        # å®Ÿè¡Œçµæœã®æ›´æ–°
        self.execution_results["end_time"] = datetime.now().isoformat()
        
        # æˆåŠŸ/å¤±æ•—ã®é›†è¨ˆ
        for category, result in self.execution_results["test_results"].items():
            if result["status"] == "PASSED":
                self.execution_results["summary"]["passed"] += 1
            elif result["status"] == "FAILED":
                self.execution_results["summary"]["failed"] += 1
            else:
                self.execution_results["summary"]["skipped"] += 1
        
        self.execution_results["summary"]["total"] = (
            self.execution_results["summary"]["passed"] + 
            self.execution_results["summary"]["failed"] + 
            self.execution_results["summary"]["skipped"]
        )
        
        # æ¨å¥¨äº‹é …ã®ç”Ÿæˆ
        recommendations = []
        
        if self.execution_results["summary"]["failed"] > 0:
            recommendations.append("å¤±æ•—ã—ãŸãƒ†ã‚¹ãƒˆã®è©³ç´°ç¢ºèªã¨ä¿®æ­£ãŒå¿…è¦")
        
        if "page_load_time" in self.execution_results["performance_metrics"]:
            load_time = self.execution_results["performance_metrics"]["page_load_time"]
            if load_time > 5.0:
                recommendations.append(f"ãƒšãƒ¼ã‚¸èª­ã¿è¾¼ã¿æ™‚é–“ãŒé…ã„ ({load_time:.1f}ç§’) - æœ€é©åŒ–ã‚’æ¤œè¨")
        
        self.execution_results["recommendations"] = recommendations
        
        # JSONãƒ¬ãƒãƒ¼ãƒˆã‚’ä¿å­˜
        report_file = os.path.join(self.results_dir, f"test_execution_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json")
        with open(report_file, "w", encoding="utf-8") as f:
            json.dump(self.execution_results, f, indent=2, ensure_ascii=False)
        
        # Markdownãƒ¬ãƒãƒ¼ãƒˆã‚‚ç”Ÿæˆ
        md_report = self._generate_markdown_report()
        md_file = os.path.join(self.results_dir, f"test_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md")
        with open(md_file, "w", encoding="utf-8") as f:
            f.write(md_report)
        
        return {
            "json_report": report_file,
            "markdown_report": md_file,
            "summary": self.execution_results["summary"]
        }
    
    def _generate_markdown_report(self):
        """Markdownå½¢å¼ã®ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        report = f"""# Chainlit UI ãƒ†ã‚¹ãƒˆå®Ÿè¡Œãƒ¬ãƒãƒ¼ãƒˆ

## ğŸ” å®Ÿè¡Œæ¦‚è¦
- **å®Ÿè¡Œæ—¥æ™‚**: {self.execution_results['start_time']}
- **ç·ãƒ†ã‚¹ãƒˆæ•°**: {self.execution_results['summary']['total']}
- **æˆåŠŸ**: âœ… {self.execution_results['summary']['passed']}
- **å¤±æ•—**: âŒ {self.execution_results['summary']['failed']} 
- **ã‚¹ã‚­ãƒƒãƒ—**: âš ï¸ {self.execution_results['summary']['skipped']}

## ğŸ“Š å®Ÿè¡Œçµæœè©³ç´°

"""
        
        for category, result in self.execution_results["test_results"].items():
            status_emoji = {"PASSED": "âœ…", "FAILED": "âŒ", "ERROR": "ğŸ’¥", "SKIPPED": "âš ï¸"}
            emoji = status_emoji.get(result["status"], "â“")
            
            report += f"### {emoji} {category}\n"
            report += f"**ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹**: {result['status']}\n\n"
            
            if "details" in result:
                report += "**è©³ç´°çµæœ**:\n"
                for key, value in result["details"].items():
                    if isinstance(value, bool):
                        value_str = "âœ… OK" if value else "âŒ NG"
                    elif isinstance(value, (int, float)):
                        value_str = f"{value}"
                    else:
                        value_str = str(value)
                    report += f"- {key}: {value_str}\n"
                report += "\n"
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æƒ…å ±
        if self.execution_results["performance_metrics"]:
            report += "## âš¡ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹\n\n"
            for metric, value in self.execution_results["performance_metrics"].items():
                report += f"- **{metric}**: {value}\n"
            report += "\n"
        
        # æ¨å¥¨äº‹é …
        if self.execution_results.get("recommendations"):
            report += "## ğŸ’¡ æ¨å¥¨äº‹é …\n\n"
            for rec in self.execution_results["recommendations"]:
                report += f"- {rec}\n"
            report += "\n"
        
        # æ‰‹å‹•ãƒ†ã‚¹ãƒˆé …ç›®
        if "manual_tests" in self.execution_results:
            report += "## ğŸ”§ æ‰‹å‹•å®Ÿè¡ŒãŒå¿…è¦ãªãƒ†ã‚¹ãƒˆ\n\n"
            report += "ä»¥ä¸‹ã®ãƒ†ã‚¹ãƒˆã¯æ‰‹å‹•ã§ã®å®Ÿè¡Œãƒ»ç¢ºèªãŒå¿…è¦ã§ã™:\n\n"
            
            for category, tests in self.execution_results["manual_tests"].items():
                if tests:
                    category_name = self.all_tests["categories"].get(category, category)
                    report += f"### {category_name}\n"
                    for test in tests:
                        report += f"- **{test['id']}**: {test['name']}\n"
                    report += "\n"
        
        return report
    
    def run_comprehensive_test(self):
        """åŒ…æ‹¬çš„ãƒ†ã‚¹ãƒˆã®å®Ÿè¡Œ"""
        print("ğŸš€ åŒ…æ‹¬çš„ãƒ†ã‚¹ãƒˆå®Ÿè¡Œã‚’é–‹å§‹ã—ã¾ã™...\n")
        
        # å‰ææ¡ä»¶ãƒã‚§ãƒƒã‚¯
        if not self.check_prerequisites():
            return None
        
        # è‡ªå‹•å®Ÿè¡Œå¯èƒ½ãªãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ
        self.execute_ui_tests()
        
        # æ‰‹å‹•ãƒ†ã‚¹ãƒˆé …ç›®ã‚’æº–å‚™
        self.execute_manual_tests()
        
        # ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆç”Ÿæˆ
        checklist_file = self.generate_test_checklist()
        
        # æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        report_info = self.generate_final_report()
        
        print(f"\nâœ… ãƒ†ã‚¹ãƒˆå®Ÿè¡Œå®Œäº†ï¼")
        print(f"ğŸ“ çµæœãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {self.results_dir}")
        print(f"ğŸ“„ å®Ÿè¡Œãƒ¬ãƒãƒ¼ãƒˆ: {report_info['markdown_report']}")
        print(f"ğŸ“‹ æ‰‹å‹•ãƒ†ã‚¹ãƒˆãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ: {checklist_file}")
        print(f"ğŸ“Š æˆåŠŸ: {report_info['summary']['passed']} / å¤±æ•—: {report_info['summary']['failed']}")
        
        return report_info


if __name__ == "__main__":
    # åŒ…æ‹¬çš„ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆã®ç”Ÿæˆï¼ˆåˆå›ã®ã¿ï¼‰
    print("ğŸ“‹ ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆã‚’æº–å‚™ä¸­...")
    test_suite = ComprehensiveTestSuite()
    test_suite.save_test_suite()
    
    # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    executor = TestExecutor()
    result = executor.run_comprehensive_test()
    
    if result:
        print(f"\nğŸ‰ ãƒ†ã‚¹ãƒˆå®Œäº†ï¼è©³ç´°ã¯ä»¥ä¸‹ã‚’ç¢ºèª:")
        print(f"   {result['markdown_report']}")