"""
ãƒ‡ãƒ¼ã‚¿ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆãƒ»ã‚¤ãƒ³ãƒãƒ¼ãƒˆãƒãƒ³ãƒ‰ãƒ©ãƒ¼
ä¼šè©±å±¥æ­´ã€ãƒšãƒ«ã‚½ãƒŠã€è¨­å®šã€çµ±è¨ˆãƒ‡ãƒ¼ã‚¿ã®ç®¡ç†æ©Ÿèƒ½
"""

import os
import json
import csv
import sqlite3
import zipfile
from datetime import datetime
from typing import Dict, List, Optional, Any, Union
from io import StringIO
import tempfile
from utils.ui_helper import ChainlitHelper as ui
from utils.error_handler import ErrorHandler as error_handler
from utils.logger import app_logger
from utils.analytics_logger import analytics_logger
from utils.persona_manager import persona_manager


class DataHandler:
    """ãƒ‡ãƒ¼ã‚¿ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆãƒ»ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚’çµ±æ‹¬ã™ã‚‹ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        """ãƒ‡ãƒ¼ã‚¿ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‚’åˆæœŸåŒ–"""
        self.chainlit_db_path = ".chainlit/chainlit.db"
        self.analytics_db_path = ".chainlit/analytics.db"
        self.personas_path = ".chainlit/personas.json"
        self.export_formats = ["json", "csv", "zip", "backup"]
    
    async def export_workspace_data(
        self,
        user_id: str,
        export_format: str = "json",
        include_conversations: bool = True,
        include_personas: bool = True,
        include_analytics: bool = True,
        date_range_days: int = 30
    ) -> Optional[str]:
        """ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã®åŒ…æ‹¬çš„ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ"""
        try:
            loading_msg = await ui.show_loading_message("ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆä¸­...")
            
            if export_format not in self.export_formats:
                await ui.update_loading_message(
                    loading_msg,
                    f"âŒ ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ãªã„å½¢å¼: {export_format}\n"
                    f"åˆ©ç”¨å¯èƒ½å½¢å¼: {', '.join(self.export_formats)}"
                )
                return None
            
            export_data = await self._collect_export_data(
                user_id, include_conversations, include_personas, include_analytics, date_range_days
            )
            
            # ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆåˆ¥ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
            if export_format == "json":
                file_path = await self._export_as_json(export_data, user_id)
            elif export_format == "csv":
                file_path = await self._export_as_csv(export_data, user_id)
            elif export_format == "zip":
                file_path = await self._export_as_zip(export_data, user_id)
            elif export_format == "backup":
                file_path = await self._create_full_backup(user_id)
            else:
                file_path = None
            
            if file_path:
                file_size = os.path.getsize(file_path)
                await ui.update_loading_message(
                    loading_msg,
                    f"âœ… ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆå®Œäº†\n\n"
                    f"**ãƒ•ã‚¡ã‚¤ãƒ«**: {os.path.basename(file_path)}\n"
                    f"**ã‚µã‚¤ã‚º**: {ui.format_file_size(file_size)}\n"
                    f"**å½¢å¼**: {export_format.upper()}\n\n"
                    f"ãƒ•ã‚¡ã‚¤ãƒ«ã¯ `.chainlit/exports/` ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ä¿å­˜ã•ã‚Œã¾ã—ãŸã€‚"
                )
                
                app_logger.info("ãƒ‡ãƒ¼ã‚¿ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆå®Œäº†", user_id=user_id, format=export_format, file_size=file_size)
                return file_path
            else:
                await ui.update_loading_message(loading_msg, "âŒ ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã«å¤±æ•—ã—ã¾ã—ãŸ")
                return None
            
        except Exception as e:
            await error_handler.handle_file_error(e, "ãƒ‡ãƒ¼ã‚¿ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ")
            return None
    
    async def export_conversations_only(
        self,
        user_id: str,
        export_format: str = "json",
        days: int = 30,
        include_metadata: bool = True
    ) -> Optional[str]:
        """ä¼šè©±å±¥æ­´ã®ã¿ã‚’ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ"""
        try:
            loading_msg = await ui.show_loading_message("ä¼šè©±å±¥æ­´ã‚’ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆä¸­...")
            
            conversations = await self._get_user_conversations(user_id, days, include_metadata)
            
            if not conversations:
                await ui.update_loading_message(
                    loading_msg,
                    "ğŸ“ ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆå¯¾è±¡ã®ä¼šè©±å±¥æ­´ãŒã‚ã‚Šã¾ã›ã‚“ã€‚"
                )
                return None
            
            # ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ç¢ºä¿
            export_dir = ".chainlit/exports"
            os.makedirs(export_dir, exist_ok=True)
            
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            
            if export_format == "json":
                filename = f"conversations_{user_id}_{timestamp}.json"
                file_path = os.path.join(export_dir, filename)
                
                with open(file_path, 'w', encoding='utf-8') as f:
                    json.dump({
                        "export_info": {
                            "user_id": user_id,
                            "export_date": datetime.now().isoformat(),
                            "days_range": days,
                            "total_conversations": len(conversations)
                        },
                        "conversations": conversations
                    }, f, ensure_ascii=False, indent=2)
                    
            elif export_format == "csv":
                filename = f"conversations_{user_id}_{timestamp}.csv"
                file_path = os.path.join(export_dir, filename)
                
                with open(file_path, 'w', newline='', encoding='utf-8') as f:
                    writer = csv.writer(f)
                    writer.writerow([
                        "ID", "ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—", "ã‚¿ã‚¤ãƒ—", "å†…å®¹", "ãƒšãƒ«ã‚½ãƒŠ", "ãƒ¢ãƒ‡ãƒ«", "ã‚»ãƒƒã‚·ãƒ§ãƒ³ID"
                    ])
                    
                    for conv in conversations:
                        writer.writerow([
                            conv.get("id", ""),
                            conv.get("timestamp", ""),
                            conv.get("type", ""),
                            conv.get("content", "").replace("\n", " "),
                            conv.get("metadata", {}).get("persona", ""),
                            conv.get("metadata", {}).get("model", ""),
                            conv.get("session_id", "")
                        ])
            else:
                await ui.update_loading_message(loading_msg, f"âŒ ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ãªã„å½¢å¼: {export_format}")
                return None
            
            file_size = os.path.getsize(file_path)
            await ui.update_loading_message(
                loading_msg,
                f"âœ… ä¼šè©±å±¥æ­´ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆå®Œäº†\n\n"
                f"**ä»¶æ•°**: {len(conversations)}ä»¶ã®ä¼šè©±\n"
                f"**æœŸé–“**: ç›´è¿‘{days}æ—¥é–“\n"
                f"**ãƒ•ã‚¡ã‚¤ãƒ«**: {filename}\n"
                f"**ã‚µã‚¤ã‚º**: {ui.format_file_size(file_size)}"
            )
            
            return file_path
            
        except Exception as e:
            await error_handler.handle_file_error(e, "ä¼šè©±å±¥æ­´ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ")
            return None
    
    async def export_analytics_data(
        self,
        user_id: str = None,
        days: int = 30,
        export_format: str = "json"
    ) -> Optional[str]:
        """çµ±è¨ˆãƒ‡ãƒ¼ã‚¿ã‚’ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ"""
        try:
            loading_msg = await ui.show_loading_message("çµ±è¨ˆãƒ‡ãƒ¼ã‚¿ã‚’ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆä¸­...")
            
            # çµ±è¨ˆãƒ‡ãƒ¼ã‚¿ã®åé›†
            usage_summary = analytics_logger.get_usage_summary(user_id, days)
            vs_summary = analytics_logger.get_vector_store_summary(user_id, days)
            
            if not usage_summary and not vs_summary:
                await ui.update_loading_message(
                    loading_msg,
                    "ğŸ“Š ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆå¯¾è±¡ã®çµ±è¨ˆãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚"
                )
                return None
            
            # ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ã®æ§‹ç¯‰
            analytics_data = {
                "export_info": {
                    "user_id": user_id,
                    "export_date": datetime.now().isoformat(),
                    "days_range": days,
                    "data_types": ["usage_summary", "vector_store_summary"]
                },
                "usage_summary": usage_summary,
                "vector_store_summary": vs_summary
            }
            
            # ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆå®Ÿè¡Œ
            export_dir = ".chainlit/exports"
            os.makedirs(export_dir, exist_ok=True)
            
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            user_suffix = f"_{user_id}" if user_id else "_all_users"
            filename = f"analytics{user_suffix}_{timestamp}.{export_format}"
            file_path = os.path.join(export_dir, filename)
            
            if export_format == "json":
                with open(file_path, 'w', encoding='utf-8') as f:
                    json.dump(analytics_data, f, ensure_ascii=False, indent=2)
            else:
                # CSVå½¢å¼ã§ã®çµ±è¨ˆãƒ‡ãƒ¼ã‚¿ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
                await ui.update_loading_message(loading_msg, "âŒ çµ±è¨ˆãƒ‡ãƒ¼ã‚¿ã®CSVå½¢å¼ã¯ç¾åœ¨ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã¾ã›ã‚“")
                return None
            
            file_size = os.path.getsize(file_path)
            await ui.update_loading_message(
                loading_msg,
                f"âœ… çµ±è¨ˆãƒ‡ãƒ¼ã‚¿ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆå®Œäº†\n\n"
                f"**æœŸé–“**: ç›´è¿‘{days}æ—¥é–“\n"
                f"**å¯¾è±¡**: {'æŒ‡å®šãƒ¦ãƒ¼ã‚¶ãƒ¼' if user_id else 'å…¨ãƒ¦ãƒ¼ã‚¶ãƒ¼'}\n"
                f"**ãƒ•ã‚¡ã‚¤ãƒ«**: {filename}\n"
                f"**ã‚µã‚¤ã‚º**: {ui.format_file_size(file_size)}"
            )
            
            return file_path
            
        except Exception as e:
            await error_handler.handle_file_error(e, "çµ±è¨ˆãƒ‡ãƒ¼ã‚¿ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ")
            return None
    
    async def import_workspace_data(
        self,
        file_path: str,
        user_id: str,
        merge_mode: bool = True
    ) -> bool:
        """ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ"""
        try:
            if not os.path.exists(file_path):
                await ui.send_error_message(f"ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {file_path}")
                return False
            
            loading_msg = await ui.show_loading_message("ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆä¸­...")
            
            # ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ã®åˆ¤å®š
            if file_path.endswith('.json'):
                success = await self._import_from_json(file_path, user_id, merge_mode)
            elif file_path.endswith('.zip'):
                success = await self._import_from_zip(file_path, user_id, merge_mode)
            else:
                await ui.update_loading_message(
                    loading_msg,
                    "âŒ ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ãªã„ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ã§ã™ã€‚JSON ã¾ãŸã¯ ZIP ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚"
                )
                return False
            
            if success:
                await ui.update_loading_message(
                    loading_msg,
                    f"âœ… ãƒ‡ãƒ¼ã‚¿ã‚¤ãƒ³ãƒãƒ¼ãƒˆå®Œäº†\n\n"
                    f"**ãƒ•ã‚¡ã‚¤ãƒ«**: {os.path.basename(file_path)}\n"
                    f"**ãƒ¢ãƒ¼ãƒ‰**: {'ãƒãƒ¼ã‚¸' if merge_mode else 'ç½®æ›'}\n\n"
                    f"ã‚¤ãƒ³ãƒãƒ¼ãƒˆã—ãŸãƒ‡ãƒ¼ã‚¿ã¯æ—¢å­˜ã®ãƒ‡ãƒ¼ã‚¿ã¨çµ±åˆã•ã‚Œã¾ã—ãŸã€‚"
                )
                
                app_logger.info("ãƒ‡ãƒ¼ã‚¿ã‚¤ãƒ³ãƒãƒ¼ãƒˆå®Œäº†", user_id=user_id, file_path=file_path)
            else:
                await ui.update_loading_message(loading_msg, "âŒ ãƒ‡ãƒ¼ã‚¿ã‚¤ãƒ³ãƒãƒ¼ãƒˆã«å¤±æ•—ã—ã¾ã—ãŸ")
            
            return success
            
        except Exception as e:
            await error_handler.handle_file_error(e, "ãƒ‡ãƒ¼ã‚¿ã‚¤ãƒ³ãƒãƒ¼ãƒˆ", os.path.basename(file_path))
            return False
    
    async def create_backup(self, user_id: str = None) -> Optional[str]:
        """å®Œå…¨ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã®ä½œæˆ"""
        try:
            loading_msg = await ui.show_loading_message("å®Œå…¨ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚’ä½œæˆä¸­...")
            
            backup_path = await self._create_full_backup(user_id)
            
            if backup_path:
                file_size = os.path.getsize(backup_path)
                await ui.update_loading_message(
                    loading_msg,
                    f"âœ… ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆå®Œäº†\n\n"
                    f"**ãƒ•ã‚¡ã‚¤ãƒ«**: {os.path.basename(backup_path)}\n"
                    f"**ã‚µã‚¤ã‚º**: {ui.format_file_size(file_size)}\n\n"
                    f"ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ•ã‚¡ã‚¤ãƒ«ã«ã¯ä»¥ä¸‹ãŒå«ã¾ã‚Œã¦ã„ã¾ã™:\n"
                    f"- ä¼šè©±å±¥æ­´ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹\n"
                    f"- çµ±è¨ˆãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹\n"
                    f"- ãƒšãƒ«ã‚½ãƒŠè¨­å®š\n"
                    f"- ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿"
                )
                return backup_path
            else:
                await ui.update_loading_message(loading_msg, "âŒ ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆã«å¤±æ•—ã—ã¾ã—ãŸ")
                return None
                
        except Exception as e:
            await error_handler.handle_file_error(e, "ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆ")
            return None
    
    async def _collect_export_data(
        self,
        user_id: str,
        include_conversations: bool,
        include_personas: bool,
        include_analytics: bool,
        date_range_days: int
    ) -> Dict[str, Any]:
        """ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆç”¨ãƒ‡ãƒ¼ã‚¿ã®åé›†"""
        export_data = {
            "export_info": {
                "user_id": user_id,
                "export_date": datetime.now().isoformat(),
                "date_range_days": date_range_days,
                "included_data": []
            }
        }
        
        if include_conversations:
            conversations = await self._get_user_conversations(user_id, date_range_days)
            export_data["conversations"] = conversations
            export_data["export_info"]["included_data"].append("conversations")
        
        if include_personas:
            personas = await persona_manager.get_all_personas()
            export_data["personas"] = personas
            export_data["export_info"]["included_data"].append("personas")
        
        if include_analytics:
            usage_summary = analytics_logger.get_usage_summary(user_id, date_range_days)
            vs_summary = analytics_logger.get_vector_store_summary(user_id, date_range_days)
            export_data["analytics"] = {
                "usage_summary": usage_summary,
                "vector_store_summary": vs_summary
            }
            export_data["export_info"]["included_data"].append("analytics")
        
        return export_data
    
    async def _get_user_conversations(
        self,
        user_id: str,
        days: int,
        include_metadata: bool = True
    ) -> List[Dict[str, Any]]:
        """ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ä¼šè©±å±¥æ­´ã‚’å–å¾—"""
        try:
            conversations = []
            
            # æ—¥ä»˜ç¯„å›²ã®è¨ˆç®—
            end_date = datetime.now()
            start_date = end_date - timedelta(days=days)
            
            with sqlite3.connect(self.chainlit_db_path) as conn:
                cursor = conn.cursor()
                
                sql = """
                SELECT e.id, e.type, e.content, e.created_at, s.id as session_id, s.metadata
                FROM element e
                LEFT JOIN step s ON e.step_id = s.id
                WHERE s.user_id = ? 
                AND e.created_at >= ? 
                AND e.created_at <= ?
                AND e.content IS NOT NULL
                ORDER BY e.created_at DESC
                """
                
                cursor.execute(sql, [user_id, start_date.isoformat(), end_date.isoformat()])
                rows = cursor.fetchall()
                
                for row in rows:
                    element_id, element_type, content, created_at, session_id, session_metadata = row
                    
                    conversation = {
                        "id": element_id,
                        "type": element_type,
                        "content": content,
                        "timestamp": created_at,
                        "session_id": session_id
                    }
                    
                    if include_metadata and session_metadata:
                        try:
                            metadata = json.loads(session_metadata)
                            conversation["metadata"] = {
                                "persona": metadata.get("active_persona", {}).get("name"),
                                "model": metadata.get("model"),
                                "settings": metadata.get("settings", {})
                            }
                        except (json.JSONDecodeError, AttributeError):
                            pass
                    
                    conversations.append(conversation)
            
            return conversations
            
        except Exception as e:
            app_logger.error(f"ä¼šè©±å±¥æ­´å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    async def _export_as_json(self, export_data: Dict[str, Any], user_id: str) -> str:
        """JSONå½¢å¼ã§ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ"""
        export_dir = ".chainlit/exports"
        os.makedirs(export_dir, exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"workspace_{user_id}_{timestamp}.json"
        file_path = os.path.join(export_dir, filename)
        
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(export_data, f, ensure_ascii=False, indent=2)
        
        return file_path
    
    async def _export_as_csv(self, export_data: Dict[str, Any], user_id: str) -> str:
        """CSVå½¢å¼ã§ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆï¼ˆè¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ«ï¼‰"""
        export_dir = ".chainlit/exports"
        os.makedirs(export_dir, exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ä¼šè©±å±¥æ­´CSV
        if "conversations" in export_data:
            conv_filename = f"conversations_{user_id}_{timestamp}.csv"
            conv_path = os.path.join(export_dir, conv_filename)
            
            with open(conv_path, 'w', newline='', encoding='utf-8') as f:
                writer = csv.writer(f)
                writer.writerow(["ID", "ã‚¿ã‚¤ãƒ—", "å†…å®¹", "ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—", "ã‚»ãƒƒã‚·ãƒ§ãƒ³ID"])
                
                for conv in export_data["conversations"]:
                    writer.writerow([
                        conv.get("id", ""),
                        conv.get("type", ""),
                        conv.get("content", "").replace("\n", " "),
                        conv.get("timestamp", ""),
                        conv.get("session_id", "")
                    ])
        
        # ãƒšãƒ«ã‚½ãƒŠCSV
        if "personas" in export_data:
            persona_filename = f"personas_{user_id}_{timestamp}.csv"
            persona_path = os.path.join(export_dir, persona_filename)
            
            with open(persona_path, 'w', newline='', encoding='utf-8') as f:
                writer = csv.writer(f)
                writer.writerow(["åå‰", "èª¬æ˜", "ãƒ¢ãƒ‡ãƒ«", "Temperature", "ã‚¿ã‚°"])
                
                for persona in export_data["personas"]:
                    writer.writerow([
                        persona.get("name", ""),
                        persona.get("description", ""),
                        persona.get("model", ""),
                        persona.get("temperature", ""),
                        ",".join(persona.get("tags", []))
                    ])
        
        # ä¸»è¦ãªãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’è¿”ã™ï¼ˆä¼šè©±å±¥æ­´ã‚’å„ªå…ˆï¼‰
        return conv_path if "conversations" in export_data else persona_path
    
    async def _export_as_zip(self, export_data: Dict[str, Any], user_id: str) -> str:
        """ZIPå½¢å¼ã§åŒ…æ‹¬çš„ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ"""
        export_dir = ".chainlit/exports"
        os.makedirs(export_dir, exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        zip_filename = f"workspace_{user_id}_{timestamp}.zip"
        zip_path = os.path.join(export_dir, zip_filename)
        
        with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
            zipf.writestr("export_info.json", json.dumps(export_data["export_info"], indent=2))
            
            # ä¼šè©±å±¥æ­´
            if "conversations" in export_data:
                zipf.writestr("conversations.json", 
                            json.dumps(export_data["conversations"], ensure_ascii=False, indent=2))
            
            # ãƒšãƒ«ã‚½ãƒŠ
            if "personas" in export_data:
                zipf.writestr("personas.json",
                            json.dumps(export_data["personas"], ensure_ascii=False, indent=2))
            
            # çµ±è¨ˆãƒ‡ãƒ¼ã‚¿
            if "analytics" in export_data:
                zipf.writestr("analytics.json",
                            json.dumps(export_data["analytics"], ensure_ascii=False, indent=2))
        
        return zip_path
    
    async def _create_full_backup(self, user_id: str = None) -> str:
        """å®Œå…¨ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ•ã‚¡ã‚¤ãƒ«ã®ä½œæˆ"""
        export_dir = ".chainlit/exports"
        os.makedirs(export_dir, exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        user_suffix = f"_{user_id}" if user_id else "_full"
        backup_filename = f"backup{user_suffix}_{timestamp}.zip"
        backup_path = os.path.join(export_dir, backup_filename)
        
        with zipfile.ZipFile(backup_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
            if os.path.exists(self.chainlit_db_path):
                zipf.write(self.chainlit_db_path, "chainlit.db")
            
            if os.path.exists(self.analytics_db_path):
                zipf.write(self.analytics_db_path, "analytics.db")
            
            # ãƒšãƒ«ã‚½ãƒŠãƒ•ã‚¡ã‚¤ãƒ«
            if os.path.exists(self.personas_path):
                zipf.write(self.personas_path, "personas.json")
            
            # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—æƒ…å ±
            backup_info = {
                "backup_date": datetime.now().isoformat(),
                "user_id": user_id,
                "backup_type": "full_backup",
                "included_files": ["chainlit.db", "analytics.db", "personas.json"]
            }
            zipf.writestr("backup_info.json", json.dumps(backup_info, indent=2))
        
        return backup_path
    
    async def _import_from_json(self, file_path: str, user_id: str, merge_mode: bool) -> bool:
        """JSONãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                import_data = json.load(f)
            
            # ãƒ‡ãƒ¼ã‚¿ã®å¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯
            if "export_info" not in import_data:
                await ui.send_error_message("ç„¡åŠ¹ãªã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ã§ã™ã€‚")
                return False
            
            # ãƒšãƒ«ã‚½ãƒŠã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
            if "personas" in import_data:
                await self._import_personas(import_data["personas"], merge_mode)
            
            # ä¼šè©±å±¥æ­´ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆç°¡æ˜“å®Ÿè£…ï¼‰
            if "conversations" in import_data:
                app_logger.info(f"ä¼šè©±å±¥æ­´ {len(import_data['conversations'])} ä»¶ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚’ã‚¹ã‚­ãƒƒãƒ—")
                # æ³¨æ„: ä¼šè©±å±¥æ­´ã®ç›´æ¥ã‚¤ãƒ³ãƒãƒ¼ãƒˆã¯è¤‡é›‘ãªãŸã‚ã€ç¾åœ¨ã¯æœªå®Ÿè£…
            
            return True
            
        except Exception as e:
            app_logger.error(f"JSONã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    async def _import_from_zip(self, file_path: str, user_id: str, merge_mode: bool) -> bool:
        """ZIPãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ"""
        try:
            with tempfile.TemporaryDirectory() as temp_dir:
                with zipfile.ZipFile(file_path, 'r') as zipf:
                    zipf.extractall(temp_dir)
                
                # ãƒšãƒ«ã‚½ãƒŠã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
                personas_path = os.path.join(temp_dir, "personas.json")
                if os.path.exists(personas_path):
                    with open(personas_path, 'r', encoding='utf-8') as f:
                        personas_data = json.load(f)
                    await self._import_personas(personas_data, merge_mode)
                
                return True
                
        except Exception as e:
            app_logger.error(f"ZIPã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    async def _import_personas(self, personas_data: List[Dict], merge_mode: bool):
        """ãƒšãƒ«ã‚½ãƒŠãƒ‡ãƒ¼ã‚¿ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ"""
        try:
            imported_count = 0
            
            for persona_data in personas_data:
                persona_name = persona_data.get("name", "")
                
                if merge_mode:
                    # æ—¢å­˜ãƒã‚§ãƒƒã‚¯
                    existing_personas = await persona_manager.get_all_personas()
                    exists = any(p.get("name") == persona_name for p in existing_personas)
                    
                    if not exists:
                        await persona_manager.create_persona(persona_data)
                        imported_count += 1
                else:
                    # å¼·åˆ¶ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆåå‰é‡è¤‡æ™‚ã¯æ–°ã—ã„åå‰ç”Ÿæˆï¼‰
                    await persona_manager.create_persona(persona_data)
                    imported_count += 1
            
            app_logger.info(f"ãƒšãƒ«ã‚½ãƒŠã‚¤ãƒ³ãƒãƒ¼ãƒˆå®Œäº†: {imported_count}ä»¶")
            
        except Exception as e:
            app_logger.error(f"ãƒšãƒ«ã‚½ãƒŠã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {e}")


# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
data_handler = DataHandler()