"""
OpenAI Responses APIç®¡ç†ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
- Responses APIã®å‘¼ã³å‡ºã—ï¼ˆWebæ¤œç´¢ã€ãƒ•ã‚¡ã‚¤ãƒ«æ¤œç´¢å¯¾å¿œï¼‰
- ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å¿œç­”å‡¦ç†
- Toolsæ©Ÿèƒ½ï¼ˆWebæ¤œç´¢ã€ãƒ•ã‚¡ã‚¤ãƒ«æ¤œç´¢ã€ã‚«ã‚¹ã‚¿ãƒ é–¢æ•°ï¼‰
- ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å±¥æ­´ç®¡ç†
- ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
"""

import os
import json
import base64
from typing import Dict, List, Optional, AsyncGenerator, Any, Union, Literal
from openai import OpenAI, AsyncOpenAI
import httpx
from datetime import datetime
import asyncio
from pathlib import Path


class ResponseHandler:
    """OpenAI Responses APIç®¡ç†ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        """åˆæœŸåŒ–"""
        self.api_key = os.getenv("OPENAI_API_KEY", "")
        self.default_model = os.getenv("DEFAULT_MODEL", "gpt-4o-mini")
        self.client = None
        self.async_client = None
        
        # ãƒ„ãƒ¼ãƒ«è¨­å®šï¼ˆç’°å¢ƒå¤‰æ•°ã¾ãŸã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ï¼‰
        self.enable_web_search = os.getenv("ENABLE_WEB_SEARCH", "false").lower() == "true"
        self.enable_file_search = os.getenv("ENABLE_FILE_SEARCH", "false").lower() == "true"
        self.enable_function_calling = os.getenv("ENABLE_FUNCTION_CALLING", "true").lower() == "true"
        
        self._init_clients()
    
    def _init_clients(self):
        """OpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’åˆæœŸåŒ–"""
        if not self.api_key or self.api_key == "your_api_key_here":
            return
        
        # ãƒ—ãƒ­ã‚­ã‚·è¨­å®šã‚’ç¢ºèª
        http_proxy = os.getenv("HTTP_PROXY", "")
        https_proxy = os.getenv("HTTPS_PROXY", "")
        
        http_client = None
        if http_proxy or https_proxy:
            proxies = {}
            if http_proxy:
                proxies["http://"] = http_proxy
            if https_proxy:
                proxies["https://"] = https_proxy
            http_client = httpx.Client(proxies=proxies)
        
        # åŒæœŸã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
        self.client = OpenAI(
            api_key=self.api_key,
            http_client=http_client
        )
        
        # éåŒæœŸã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
        async_http_client = None
        if http_proxy or https_proxy:
            async_http_client = httpx.AsyncClient(proxies=proxies)
        
        self.async_client = AsyncOpenAI(
            api_key=self.api_key,
            http_client=async_http_client
        )
    
    def update_api_key(self, api_key: str):
        """APIã‚­ãƒ¼ã‚’æ›´æ–°"""
        self.api_key = api_key
        os.environ["OPENAI_API_KEY"] = api_key
        self._init_clients()
    
    def update_model(self, model: str):
        """ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ¢ãƒ‡ãƒ«ã‚’æ›´æ–°"""
        self.default_model = model
        os.environ["DEFAULT_MODEL"] = model
    
    def set_tool_settings(
        self,
        web_search: Optional[bool] = None,
        file_search: Optional[bool] = None,
        function_calling: Optional[bool] = None
    ):
        """
        ãƒ„ãƒ¼ãƒ«è¨­å®šã‚’æ›´æ–°
        
        Args:
            web_search: Webæ¤œç´¢ã®æœ‰åŠ¹/ç„¡åŠ¹
            file_search: ãƒ•ã‚¡ã‚¤ãƒ«æ¤œç´¢ã®æœ‰åŠ¹/ç„¡åŠ¹
            function_calling: Function callingã®æœ‰åŠ¹/ç„¡åŠ¹
        """
        if web_search is not None:
            self.enable_web_search = web_search
            os.environ["ENABLE_WEB_SEARCH"] = str(web_search).lower()
        
        if file_search is not None:
            self.enable_file_search = file_search
            os.environ["ENABLE_FILE_SEARCH"] = str(file_search).lower()
        
        if function_calling is not None:
            self.enable_function_calling = function_calling
            os.environ["ENABLE_FUNCTION_CALLING"] = str(function_calling).lower()
    
    async def create_response(
        self,
        input: Union[str, List[Dict[str, Any]]],
        model: str = None,
        instructions: str = None,
        tools: Optional[List[Dict]] = None,
        file_ids: Optional[List[str]] = None,
        metadata: Optional[Dict] = None,
        temperature: float = 0.7,
        max_tokens: int = None,
        stream: bool = False,
        response_format: Optional[Dict] = None,
        **kwargs
    ) -> AsyncGenerator[Dict, None]:
        """
        Responses APIã‚’å‘¼ã³å‡ºã—
        
        Args:
            input: å…¥åŠ›ï¼ˆæ–‡å­—åˆ—ã¾ãŸã¯ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸é…åˆ—ï¼‰
            model: ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«
            instructions: ã‚·ã‚¹ãƒ†ãƒ æŒ‡ç¤º
            tools: ä½¿ç”¨ã™ã‚‹ãƒ„ãƒ¼ãƒ«ï¼ˆè‡ªå‹•è¨­å®šã‚‚å¯èƒ½ï¼‰
            file_ids: æ¤œç´¢å¯¾è±¡ã®ãƒ•ã‚¡ã‚¤ãƒ«ID
            metadata: ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
            temperature: å‰µé€ æ€§ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
            max_tokens: æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°
            stream: ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°æœ‰åŠ¹/ç„¡åŠ¹
            response_format: å¿œç­”ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆï¼ˆJSON modeç­‰ï¼‰
            **kwargs: ãã®ä»–ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        
        Yields:
            Responses APIã®å¿œç­”
        """
        if not self.async_client:
            yield {
                "error": "APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“",
                "type": "configuration_error"
            }
            return
        
        model = model or self.default_model
        
        # ãƒ„ãƒ¼ãƒ«ã®è‡ªå‹•è¨­å®š
        if tools is None:
            tools = []
            
            # Webæ¤œç´¢ãƒ„ãƒ¼ãƒ«
            if self.enable_web_search:
                tools.append({
                    "type": "web_search"
                })
            
            # ãƒ•ã‚¡ã‚¤ãƒ«æ¤œç´¢ãƒ„ãƒ¼ãƒ«
            if self.enable_file_search and file_ids:
                tools.append({
                    "type": "file_search",
                    "file_search": {
                        "file_ids": file_ids
                    }
                })
        
        # APIå‘¼ã³å‡ºã—ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ§‹ç¯‰
        api_params = {
            "model": model,
            "input": input,
            "temperature": temperature,
            "stream": stream,
            **kwargs
        }
        
        # ã‚ªãƒ—ã‚·ãƒ§ãƒ³ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¿½åŠ 
        if instructions:
            api_params["instructions"] = instructions
        if tools:
            api_params["tools"] = tools
        if metadata:
            api_params["metadata"] = metadata
        if max_tokens:
            api_params["max_tokens"] = max_tokens
        if response_format:
            api_params["response_format"] = response_format
        
        try:
            # Responses APIå‘¼ã³å‡ºã—
            response = await self.async_client.responses.create(**api_params)
            
            if stream:
                # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ¢ãƒ¼ãƒ‰
                async for chunk in response:
                    yield self._format_response_chunk(chunk)
            else:
                # éã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ¢ãƒ¼ãƒ‰
                yield self._format_response(response)
        
        except AttributeError:
            # Responses APIãŒã¾ã åˆ©ç”¨ã§ããªã„å ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            # Chat Completions APIã‚’ä½¿ç”¨
            yield await self._fallback_to_chat_completions(
                input, model, instructions, tools, temperature, 
                max_tokens, stream, response_format, **kwargs
            )
        
        except Exception as e:
            yield {
                "error": str(e),
                "type": "api_error"
            }
    
    async def _fallback_to_chat_completions(
        self,
        input: Union[str, List[Dict[str, Any]]],
        model: str,
        instructions: str,
        tools: Optional[List[Dict]],
        temperature: float,
        max_tokens: int,
        stream: bool,
        response_format: Optional[Dict],
        **kwargs
    ):
        """
        Responses APIãŒåˆ©ç”¨ã§ããªã„å ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        Chat Completions APIã‚’ä½¿ç”¨
        """
        # å…¥åŠ›ã‚’ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å½¢å¼ã«å¤‰æ›
        if isinstance(input, str):
            messages = [
                {"role": "system", "content": instructions or "You are a helpful assistant."},
                {"role": "user", "content": input}
            ]
        else:
            messages = []
            if instructions:
                messages.append({"role": "system", "content": instructions})
            messages.extend(input)
        
        # Chat Completionsç”¨ã®ãƒ„ãƒ¼ãƒ«å½¢å¼ã«å¤‰æ›
        chat_tools = []
        for tool in (tools or []):
            if tool["type"] == "web_search":
                chat_tools.append({
                    "type": "function",
                    "function": {
                        "name": "web_search",
                        "description": "Search the web for information",
                        "parameters": {
                            "type": "object",
                            "properties": {
                                "query": {
                                    "type": "string",
                                    "description": "The search query"
                                }
                            },
                            "required": ["query"]
                        }
                    }
                })
            elif tool["type"] == "file_search":
                chat_tools.append({
                    "type": "function",
                    "function": {
                        "name": "file_search",
                        "description": "Search within uploaded files",
                        "parameters": {
                            "type": "object",
                            "properties": {
                                "query": {
                                    "type": "string",
                                    "description": "The search query"
                                },
                                "file_ids": {
                                    "type": "array",
                                    "items": {"type": "string"},
                                    "description": "File IDs to search within"
                                }
                            },
                            "required": ["query"]
                        }
                    }
                })
        
        # Chat Completions APIã‚’å‘¼ã³å‡ºã—
        api_params = {
            "model": model,
            "messages": messages,
            "temperature": temperature,
            "stream": stream
        }
        
        if chat_tools:
            api_params["tools"] = chat_tools
            api_params["tool_choice"] = "auto"
        if max_tokens:
            api_params["max_tokens"] = max_tokens
        if response_format:
            api_params["response_format"] = response_format
        
        response = await self.async_client.chat.completions.create(**api_params)
        
        if stream:
            # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å¿œç­”ã‚’å‡¦ç†
            full_content = ""
            async for chunk in response:
                if chunk.choices and chunk.choices[0].delta.content:
                    full_content += chunk.choices[0].delta.content
                    yield {
                        "type": "stream_chunk",
                        "content": chunk.choices[0].delta.content
                    }
            
            # æœ€çµ‚çš„ãªå¿œç­”ã‚’è¿”ã™
            return {
                "id": "fallback_response",
                "object": "response",
                "output_text": full_content,
                "model": model,
                "created_at": datetime.now().timestamp()
            }
        else:
            # éã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å¿œç­”
            content = response.choices[0].message.content if response.choices else ""
            tool_calls = response.choices[0].message.tool_calls if response.choices and hasattr(response.choices[0].message, 'tool_calls') else None
            
            return {
                "id": response.id,
                "object": "response",
                "output_text": content,
                "model": model,
                "created_at": response.created,
                "tool_calls": tool_calls
            }
    
    def _format_response(self, response) -> Dict:
        """
        Responses APIã®å¿œç­”ã‚’ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
        """
        return {
            "id": response.id,
            "object": response.object,
            "created_at": response.created_at,
            "model": response.model,
            "output": response.output,
            "output_text": response.output_text if hasattr(response, 'output_text') else None,
            "instructions": response.instructions,
            "metadata": response.metadata,
            "error": response.error,
            "incomplete_details": response.incomplete_details
        }
    
    def _format_response_chunk(self, chunk) -> Dict:
        """
        ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒãƒ£ãƒ³ã‚¯ã‚’ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
        """
        return {
            "type": "stream_chunk",
            "id": chunk.id if hasattr(chunk, 'id') else None,
            "content": chunk.content if hasattr(chunk, 'content') else None,
            "delta": chunk.delta if hasattr(chunk, 'delta') else None
        }
    
    async def upload_file(
        self,
        file_path: str,
        purpose: Literal["assistants", "vision", "batch", "fine-tune"] = "assistants"
    ) -> Optional[str]:
        """
        ãƒ•ã‚¡ã‚¤ãƒ«ã‚’OpenAIã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
        
        Args:
            file_path: ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
            purpose: ãƒ•ã‚¡ã‚¤ãƒ«ã®ç”¨é€”
        
        Returns:
            ãƒ•ã‚¡ã‚¤ãƒ«ID
        """
        if not self.async_client:
            print("âŒ APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
            return None
        
        try:
            with open(file_path, "rb") as file:
                response = await self.async_client.files.create(
                    file=file,
                    purpose=purpose
                )
                return response.id
        except Exception as e:
            print(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    async def delete_file(self, file_id: str) -> bool:
        """
        ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
        
        Args:
            file_id: å‰Šé™¤ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ã®ID
        
        Returns:
            å‰Šé™¤æˆåŠŸã®å¯å¦
        """
        if not self.async_client:
            return False
        
        try:
            await self.async_client.files.delete(file_id)
            return True
        except Exception as e:
            print(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤ã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    async def list_files(self) -> List[Dict]:
        """
        ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ã‚’å–å¾—
        
        Returns:
            ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±ã®ãƒªã‚¹ãƒˆ
        """
        if not self.async_client:
            return []
        
        try:
            files = await self.async_client.files.list()
            return [
                {
                    "id": f.id,
                    "filename": f.filename,
                    "purpose": f.purpose,
                    "created_at": f.created_at,
                    "bytes": f.bytes
                }
                for f in files
            ]
        except Exception as e:
            print(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    def encode_file_to_base64(self, file_path: str) -> Optional[str]:
        """
        ãƒ•ã‚¡ã‚¤ãƒ«ã‚’Base64ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ï¼ˆç”»åƒã‚„PDFç”¨ï¼‰
        
        Args:
            file_path: ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        
        Returns:
            Base64ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã•ã‚ŒãŸæ–‡å­—åˆ—
        """
        try:
            with open(file_path, "rb") as file:
                return base64.b64encode(file.read()).decode('utf-8')
        except Exception as e:
            print(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def create_image_input(self, image_path: str) -> Dict:
        """
        ç”»åƒå…¥åŠ›ã‚’ä½œæˆ
        
        Args:
            image_path: ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        
        Returns:
            ç”»åƒå…¥åŠ›ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
        """
        base64_image = self.encode_file_to_base64(image_path)
        if not base64_image:
            return {}
        
        # MIMEã‚¿ã‚¤ãƒ—ã‚’åˆ¤å®š
        suffix = Path(image_path).suffix.lower()
        mime_types = {
            '.jpg': 'image/jpeg',
            '.jpeg': 'image/jpeg',
            '.png': 'image/png',
            '.gif': 'image/gif',
            '.webp': 'image/webp'
        }
        mime_type = mime_types.get(suffix, 'image/jpeg')
        
        return {
            "type": "input_image",
            "image_url": f"data:{mime_type};base64,{base64_image}"
        }
    
    def create_pdf_input(self, pdf_path: str) -> Dict:
        """
        PDFå…¥åŠ›ã‚’ä½œæˆ
        
        Args:
            pdf_path: PDFãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        
        Returns:
            PDFå…¥åŠ›ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
        """
        base64_pdf = self.encode_file_to_base64(pdf_path)
        if not base64_pdf:
            return {}
        
        return {
            "type": "input_file",
            "filename": Path(pdf_path).name,
            "file_data": f"data:application/pdf;base64,{base64_pdf}"
        }
    
    def format_messages_for_responses_api(
        self,
        messages: List[Dict[str, Any]],
        system_prompt: str = None
    ) -> Union[str, List[Dict]]:
        """
        ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å±¥æ­´ã‚’Responses APIç”¨ã«ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
        
        Args:
            messages: ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å±¥æ­´
            system_prompt: ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        
        Returns:
            Responses APIç”¨ã®å…¥åŠ›
        """
        # å˜ç´”ãªã‚±ãƒ¼ã‚¹ï¼šãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒ1ã¤ã ã‘
        if len(messages) == 1 and messages[0].get("role") == "user":
            return messages[0]["content"]
        
        # è¤‡é›‘ãªã‚±ãƒ¼ã‚¹ï¼šè¤‡æ•°ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
        formatted = []
        
        # ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒã‚ã‚‹å ´åˆã¯æœ€åˆã«è¿½åŠ 
        if system_prompt:
            formatted.append({
                "role": "system",
                "content": system_prompt
            })
        
        # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¿½åŠ 
        formatted.extend(messages)
        
        return formatted
    
    def calculate_token_estimate(self, text: str) -> int:
        """
        ãƒ†ã‚­ã‚¹ãƒˆã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’æ¨å®š
        
        Args:
            text: ãƒ†ã‚­ã‚¹ãƒˆ
        
        Returns:
            æ¨å®šãƒˆãƒ¼ã‚¯ãƒ³æ•°
        """
        # ç°¡æ˜“çš„ãªæ¨å®šï¼ˆå®Ÿéš›ã¯tiktokenã‚’ä½¿ã†ã¹ãï¼‰
        # æ—¥æœ¬èª: 1æ–‡å­— â‰ˆ 2-3ãƒˆãƒ¼ã‚¯ãƒ³
        # è‹±èª: 1å˜èª â‰ˆ 1-1.5ãƒˆãƒ¼ã‚¯ãƒ³
        # ã“ã“ã§ã¯ç°¡æ˜“çš„ã«æ–‡å­—æ•°ã®1/3ã¨ã—ã¦æ¨å®š
        return len(text) // 3
    
    async def generate_title(self, input_text: str) -> str:
        """
        å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ã‚¿ã‚¤ãƒˆãƒ«ã‚’è‡ªå‹•ç”Ÿæˆ
        
        Args:
            input_text: å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆ
        
        Returns:
            ç”Ÿæˆã•ã‚ŒãŸã‚¿ã‚¤ãƒˆãƒ«
        """
        if not self.async_client:
            return f"Chat - {datetime.now().strftime('%Y-%m-%d %H:%M')}"
        
        try:
            response = await self.create_response(
                input=f"ä»¥ä¸‹ã®ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰çŸ­ãç°¡æ½”ãªã‚¿ã‚¤ãƒˆãƒ«ã‚’æ—¥æœ¬èªã§ç”Ÿæˆã—ã¦ãã ã•ã„ï¼ˆ20æ–‡å­—ä»¥å†…ï¼‰:\n\n{input_text[:500]}",
                model="gpt-4o-mini",
                temperature=0.5,
                max_tokens=30,
                stream=False
            )
            
            # å¿œç­”ã‹ã‚‰ã‚¿ã‚¤ãƒˆãƒ«ã‚’æŠ½å‡º
            async for result in response:
                if "output_text" in result:
                    title = result["output_text"].strip()
                    if len(title) > 30:
                        title = title[:27] + "..."
                    return title
            
            return f"Chat - {datetime.now().strftime('%Y-%m-%d %H:%M')}"
        
        except Exception as e:
            print(f"Error generating title: {e}")
            return f"Chat - {datetime.now().strftime('%Y-%m-%d %H:%M')}"
    
    def format_token_usage(self, usage: Dict[str, int]) -> str:
        """
        ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡ã‚’ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
        
        Args:
            usage: ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡
        
        Returns:
            ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã•ã‚ŒãŸæ–‡å­—åˆ—
        """
        if not usage:
            return ""
        
        prompt = usage.get("prompt_tokens", 0)
        completion = usage.get("completion_tokens", 0)
        total = usage.get("total_tokens", 0)
        
        # æ¦‚ç®—ã‚³ã‚¹ãƒˆè¨ˆç®—ï¼ˆGPT-4o-miniã®æ–™é‡‘: $0.15/1M input, $0.6/1M outputï¼‰
        input_cost = prompt * 0.00000015
        output_cost = completion * 0.0000006
        total_cost = input_cost + output_cost
        
        return f"ğŸ“Š ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡: å…¥åŠ› {prompt} + å‡ºåŠ› {completion} = åˆè¨ˆ {total} (ç´„${total_cost:.4f})"


# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
response_handler = ResponseHandler()
