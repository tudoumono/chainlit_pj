"""
Chat Completions API with Toolsç®¡ç†ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«

========================================================
é‡è¦ï¼šAPIåç§°ã®æ˜ç¢ºåŒ–
========================================================

ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¯æ­´å²çš„ç†ç”±ã§"responses_handler"ã¨ã„ã†åå‰ã§ã™ãŒã€
å®Ÿéš›ã¯Chat Completions APIã®ãƒ„ãƒ¼ãƒ«æ©Ÿèƒ½ã‚’ç®¡ç†ã—ã¦ã„ã¾ã™ã€‚

OpenAIã¯2024å¹´12æœˆã«æ–°ã—ã„ãƒ„ãƒ¼ãƒ«æ©Ÿèƒ½ã‚’ç™ºè¡¨ã—ã¾ã—ãŸï¼š
- ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°åï¼š"Responses API"
- æŠ€è¡“çš„å®Ÿè£…ï¼šChat Completions APIã®æ‹¡å¼µ
- Python SDKï¼šclient.chat.completions.create()ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ç”¨

è©³ç´°ã¯ docs/API_CLARIFICATION.md ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚

========================================================
æ©Ÿèƒ½æ¦‚è¦
========================================================

- Chat Completions APIã®å‘¼ã³å‡ºã—
- Toolsæ©Ÿèƒ½ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«æ¤œç´¢ã€ã‚³ãƒ¼ãƒ‰å®Ÿè¡Œï¼‰å¯¾å¿œ
- ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å¿œç­”å‡¦ç†
- ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å±¥æ­´ç®¡ç†
- ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
- ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢çµ±åˆ

========================================================
å‚ç…§ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ
========================================================

å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ:
- Tools Documentation: https://platform.openai.com/docs/guides/tools
- File Search Guide: https://platform.openai.com/docs/guides/tools-file-search
- Chat Completions API: https://platform.openai.com/docs/api-reference/chat

ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ:
- docs/API_CLARIFICATION.md - APIåç§°ã®æ˜ç¢ºåŒ–
- 1.2_Chainlit_å¤šæ©Ÿèƒ½AIãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹_ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ä»•æ§˜æ›¸_æ›´æ–°ç‰ˆ.md

========================================================
å®Ÿè£…ä¸Šã®æ³¨æ„
========================================================

1. "Responses API"ã¨ã„ã†ç‹¬ç«‹ã—ãŸAPIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã¯å­˜åœ¨ã—ã¾ã›ã‚“
2. client.responses.create()ãƒ¡ã‚½ãƒƒãƒ‰ã¯å­˜åœ¨ã—ã¾ã›ã‚“
3. ã™ã¹ã¦ã®æ©Ÿèƒ½ã¯Chat Completions APIã‚’é€šã˜ã¦åˆ©ç”¨ã—ã¾ã™
4. ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã¯Beta APIã‚’ä½¿ç”¨ã—ã¾ã™ï¼ˆclient.beta.vector_storesï¼‰
"""

import os
import json
import asyncio
from typing import Dict, List, Optional, AsyncGenerator, Any, Union
from openai import OpenAI, AsyncOpenAI
import httpx
from datetime import datetime
from .tools_config import tools_config
from .logger import app_logger  # ãƒ­ã‚°ã‚·ã‚¹ãƒ†ãƒ ã‚’è¿½åŠ 
from .vector_store_handler import vector_store_handler  # ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‚’è¿½åŠ 


class ResponsesAPIHandler:
    """
    OpenAI Responses APIç®¡ç†ã‚¯ãƒ©ã‚¹
    
    ã“ã®ã‚¯ãƒ©ã‚¹ã¯OpenAI SDKã®Responses APIã‚’ä½¿ç”¨ã—ã¦AIå¿œç­”ã‚’ç”Ÿæˆã—ã¾ã™ã€‚
    Responses APIãŒåˆ©ç”¨ã§ããªã„å ´åˆã¯ã€Chat Completions APIã«è‡ªå‹•çš„ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã—ã¾ã™ã€‚
    
    é‡è¦: OpenAI SDKã¯Responses APIã‚’æ­£å¼ã«ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã¾ã™ã€‚
    ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã¯äº’æ›æ€§ã®ãŸã‚ã®ã‚‚ã®ã§ã‚ã‚Šã€SDKã®åˆ¶é™ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚
    """
    
    def __init__(self):
        """åˆæœŸåŒ–"""
        self.api_key = os.getenv("OPENAI_API_KEY", "")
        self.default_model = os.getenv("DEFAULT_MODEL", "gpt-4o-mini")
        self.client = None
        self.async_client = None
        self.tools_config = tools_config
        self._init_clients()
    
    def _init_clients(self):
        """OpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’åˆæœŸåŒ–"""
        if not self.api_key or self.api_key == "your_api_key_here":
            return
        
        # ãƒ—ãƒ­ã‚­ã‚·è¨­å®šã‚’ç¢ºèª
        http_proxy = os.getenv("HTTP_PROXY", "")
        https_proxy = os.getenv("HTTPS_PROXY", "")
        
        http_client = None
        if http_proxy or https_proxy:
            proxies = {}
            if http_proxy:
                proxies["http://"] = http_proxy
            if https_proxy:
                proxies["https://"] = https_proxy
            http_client = httpx.Client(proxies=proxies)
        
        # åŒæœŸã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
        self.client = OpenAI(
            api_key=self.api_key,
            http_client=http_client
        )
        
        # éåŒæœŸã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
        async_http_client = None
        if http_proxy or https_proxy:
            async_http_client = httpx.AsyncClient(proxies=proxies)
        
        self.async_client = AsyncOpenAI(
            api_key=self.api_key,
            http_client=async_http_client
        )
    
    def update_api_key(self, api_key: str):
        """APIã‚­ãƒ¼ã‚’æ›´æ–°"""
        self.api_key = api_key
        os.environ["OPENAI_API_KEY"] = api_key
        self._init_clients()
    
    def update_model(self, model: str):
        """ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ¢ãƒ‡ãƒ«ã‚’æ›´æ–°"""
        self.default_model = model
        os.environ["DEFAULT_MODEL"] = model
    
    async def create_response(
        self,
        messages: List[Dict[str, str]],
        model: str = None,
        temperature: float = 0.7,
        max_tokens: int = None,
        stream: bool = True,
        use_tools: bool = None,
        tool_choice: Union[str, Dict] = None,
        previous_response_id: str = None,
        session: Optional[Dict] = None,  # Chainlitã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’è¿½åŠ 
        **kwargs
    ) -> AsyncGenerator[Dict, None]:
        """
        Responses APIã‚’å‘¼ã³å‡ºã—ï¼ˆToolsæ©Ÿèƒ½å¯¾å¿œï¼‰
        
        é‡è¦: ã“ã®ãƒ¡ã‚½ãƒƒãƒ‰ã¯ã¾ãšResponses APIã‚’è©¦ã¿ã€åˆ©ç”¨ã§ããªã„å ´åˆã®ã¿
        Chat Completions APIã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã—ã¾ã™ã€‚ã“ã‚Œã¯SDKã®ä»•æ§˜ã«åŸºã¥ã
        æ­£ã—ã„å®Ÿè£…ãƒ‘ã‚¿ãƒ¼ãƒ³ã§ã™ã€‚
        
        å‚ç…§:
        - https://platform.openai.com/docs/api-reference/responses
        - https://platform.openai.com/docs/quickstart?api-mode=responses
        
        Args:
            messages: ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å±¥æ­´
            model: ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«
            temperature: å‰µé€ æ€§ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
            max_tokens: æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°
            stream: ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°æœ‰åŠ¹/ç„¡åŠ¹
            use_tools: Toolsæ©Ÿèƒ½ã‚’ä½¿ç”¨ã™ã‚‹ã‹
            tool_choice: ãƒ„ãƒ¼ãƒ«é¸æŠè¨­å®š
            session: Chainlitã‚»ãƒƒã‚·ãƒ§ãƒ³æƒ…å ±
            **kwargs: ãã®ä»–ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        
        Yields:
            ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒãƒ£ãƒ³ã‚¯ or å®Œäº†ãƒ¬ã‚¹ãƒãƒ³ã‚¹
        """
        if not self.async_client:
            yield {
                "error": "APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“",
                "type": "configuration_error"
            }
            return
        
        model = model or self.default_model
        
        # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å±¥æ­´ã‹ã‚‰å…¥åŠ›ã¨ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æŠ½å‡º
        input_content = None
        instructions = None
        
        # æœ€æ–°ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å–å¾—
        for msg in reversed(messages):
            if msg.get("role") == "user":
                input_content = msg.get("content")
                break
        
        # ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å–å¾—
        for msg in messages:
            if msg.get("role") == "system":
                instructions = msg.get("content")
                break
        
        # Responses APIãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ§‹ç¯‰
        response_params = {
            "model": model,
            "temperature": temperature,
            "stream": stream,
            **kwargs
        }
        
        # inputãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¨­å®šï¼ˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¾ãŸã¯æ–‡å­—åˆ—ï¼‰
        if messages:
            # æœ€æ–°ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å–å¾—ã™ã‚‹ã‹ã€å…¨ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’é€ä¿¡
            if len(messages) == 1 and messages[0].get("role") == "user":
                response_params["input"] = messages[0].get("content")
            else:
                # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸é…åˆ—ã¨ã—ã¦é€ä¿¡
                response_params["input"] = messages
        
        # instructionsã‚’è¨­å®šï¼ˆã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼‰
        for msg in messages:
            if msg.get("role") == "system":
                response_params["instructions"] = msg.get("content")
                # ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’inputã‹ã‚‰é™¤å¤–
                if isinstance(response_params.get("input"), list):
                    response_params["input"] = [m for m in response_params["input"] if m.get("role") != "system"]
                break
        
        # ä¼šè©±ç¶™ç¶šç”¨ã®response_id
        if previous_response_id:
            response_params["previous_response_id"] = previous_response_id
        
        if max_tokens:
            response_params["max_tokens"] = max_tokens
        
        # Toolsæ©Ÿèƒ½ã®è¨­å®š
        if use_tools and self.tools_config.is_enabled():
            tools = self.tools_config.build_tools_parameter(session)  # ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’æ¸¡ã™
            if tools:
                response_params["tools"] = tools
                response_params["tool_choice"] = tool_choice or "auto"
        
        # ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°ã‚’è¿½åŠ 
        app_logger.debug(f"ğŸ”§ create_responseé–‹å§‹", 
                        model=model, 
                        stream=stream,
                        tools_enabled=use_tools,
                        message_count=len(messages))
        
        response_stream = None
        try:
            # ========================================================
            # Responses APIã‚’è©¦ã™
            # é‡è¦: OpenAI SDKã¯Responses APIã‚’æ­£å¼ã«ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã¾ã™
            # å‚ç…§: https://platform.openai.com/docs/api-reference/responses
            # ========================================================
            try:
                app_logger.debug("ğŸ”§ Responses APIå‘¼ã³å‡ºã—ã‚’è©¦è¡Œä¸­...")
                response = await self.async_client.responses.create(**response_params)
                
                # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ¢ãƒ¼ãƒ‰
                if stream:
                    app_logger.debug("ğŸ”§ Responses APIã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ¢ãƒ¼ãƒ‰")
                    async for event in response:
                        yield self._process_response_stream_event(event)
                # éã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ¢ãƒ¼ãƒ‰
                else:
                    app_logger.debug("ğŸ”§ Responses APIéã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ¢ãƒ¼ãƒ‰")
                    yield self._process_response_output(response)
            
            except AttributeError as e:
                app_logger.debug(f"âš ï¸ Responses APIãŒåˆ©ç”¨ä¸å¯: {e}")
                app_logger.debug("ğŸ”§ Chat Completions APIã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯")
                # ========================================================
                # Responses APIãŒåˆ©ç”¨ã§ããªã„å ´åˆã€Chat Completions APIã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                # æ³¨æ„: ã“ã‚Œã¯SDKã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚„ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«çŠ¶æ³ã«ã‚ˆã‚‹ã‚‚ã®ã§ã™
                # OpenAI SDKã¯æ­£å¼ã«Responses APIã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã¾ã™
                # ========================================================
                chat_params = {
                    "model": model,
                    "messages": messages,
                    "temperature": temperature,
                    "stream": stream,
                    **kwargs
                }
                
                if max_tokens:
                    chat_params["max_tokens"] = max_tokens
                
                # Toolsæ©Ÿèƒ½ã®è¨­å®š
                if use_tools and self.tools_config.is_enabled():
                    tools = self.tools_config.build_tools_parameter(session)  # ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’æ¸¡ã™
                    if tools:
                        chat_params["tools"] = tools
                        chat_params["tool_choice"] = tool_choice or "auto"
                
                response_stream = await self.async_client.chat.completions.create(**chat_params)
                
                # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ¢ãƒ¼ãƒ‰
                if stream:
                    app_logger.debug("ğŸ”§ Chat Completions APIã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ¢ãƒ¼ãƒ‰é–‹å§‹")
                    try:
                        async for chunk in response_stream:
                            if chunk:  # chunkãŒNoneã§ãªã„ã“ã¨ã‚’ç¢ºèª
                                yield self._process_stream_chunk(chunk)
                    except asyncio.CancelledError:
                        app_logger.debug("âš ï¸ ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãŒã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ")
                        # Cancelled Errorã¯æ­£å¸¸ãªçµ‚äº†ã¨ã—ã¦æ‰±ã†
                        return
                    except GeneratorExit:
                        app_logger.debug("âš ï¸ ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ãƒ¼ãŒçµ‚äº†ã—ã¾ã—ãŸ")
                        # GeneratorExitã‚‚æ­£å¸¸ãªçµ‚äº†ã¨ã—ã¦æ‰±ã†
                        return
                    finally:
                        app_logger.debug("ğŸ”§ ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°çµ‚äº†å‡¦ç†")
                        # response_streamã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
                        if response_stream and hasattr(response_stream, 'aclose'):
                            try:
                                await response_stream.aclose()
                            except Exception as cleanup_error:
                                app_logger.debug(f"âš ï¸ ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚¨ãƒ©ãƒ¼: {cleanup_error}")
                # éã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ¢ãƒ¼ãƒ‰
                else:
                    app_logger.debug("ğŸ”§ Chat Completions APIéã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ¢ãƒ¼ãƒ‰")
                    yield self._process_response(response_stream)
        
        except asyncio.CancelledError:
            app_logger.debug("âš ï¸ å‡¦ç†ãŒã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ")
            # CancelledErrorã¯å†åº¦raiseã™ã‚‹å¿…è¦ãŒã‚ã‚‹
            raise
        except Exception as e:
            app_logger.error(f"âŒ APIå‘¼ã³å‡ºã—ã‚¨ãƒ©ãƒ¼: {e}")
            import traceback
            app_logger.debug(f"âŒ ã‚¨ãƒ©ãƒ¼ãƒˆãƒ¬ãƒ¼ã‚¹ãƒãƒƒã‚¯: {traceback.format_exc()}")
            yield {
                "error": str(e),
                "type": "api_error",
                "details": {
                    "model": model,
                    "tools_enabled": use_tools,
                    "error_type": type(e).__name__
                }
            }
        finally:
            app_logger.debug("ğŸ”§ create_responseçµ‚äº†")
            # ãƒªã‚½ãƒ¼ã‚¹ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            if response_stream and hasattr(response_stream, 'aclose'):
                try:
                    await response_stream.aclose()
                except Exception:
                    pass  # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚¨ãƒ©ãƒ¼ã¯ç„¡è¦–
    
    def _process_stream_chunk(self, chunk) -> Dict[str, Any]:
        """ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒãƒ£ãƒ³ã‚¯ã‚’å‡¦ç†"""
        chunk_dict = {
            "id": chunk.id,
            "model": chunk.model,
            "created": chunk.created,
            "object": "chat.completion.chunk",
            "choices": []
        }
        
        # choices ã‚’å‡¦ç†
        if chunk.choices:
            for choice in chunk.choices:
                choice_dict = {
                    "index": choice.index,
                    "delta": {}
                }
                
                # deltaã®å†…å®¹ã‚’å‡¦ç†
                if choice.delta:
                    # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„
                    if choice.delta.content is not None:
                        choice_dict["delta"]["content"] = choice.delta.content
                    
                    # ãƒ­ãƒ¼ãƒ«
                    if hasattr(choice.delta, 'role') and choice.delta.role is not None:
                        choice_dict["delta"]["role"] = choice.delta.role
                    
                    # ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—
                    if hasattr(choice.delta, 'tool_calls') and choice.delta.tool_calls:
                        choice_dict["delta"]["tool_calls"] = []
                        for tc in choice.delta.tool_calls:
                            tool_call = {
                                "index": tc.index if hasattr(tc, 'index') else None
                            }
                            
                            if hasattr(tc, 'id') and tc.id:
                                tool_call["id"] = tc.id
                            if hasattr(tc, 'type') and tc.type:
                                tool_call["type"] = tc.type
                            
                            # Webæ¤œç´¢ãƒ„ãƒ¼ãƒ«
                            if hasattr(tc, 'web_search') and tc.web_search:
                                tool_call["web_search"] = {
                                    "query": tc.web_search.query if hasattr(tc.web_search, 'query') else None
                                }
                            
                            # ãƒ•ã‚¡ã‚¤ãƒ«æ¤œç´¢ãƒ„ãƒ¼ãƒ«
                            elif hasattr(tc, 'file_search') and tc.file_search:
                                tool_call["file_search"] = {}
                            
                            # é–¢æ•°å‘¼ã³å‡ºã—
                            elif hasattr(tc, 'function') and tc.function:
                                tool_call["function"] = {
                                    "name": tc.function.name if hasattr(tc.function, 'name') else None,
                                    "arguments": tc.function.arguments if hasattr(tc.function, 'arguments') else None
                                }
                            
                            choice_dict["delta"]["tool_calls"].append(tool_call)
                
                # finish_reasonã‚’å‡¦ç†
                if hasattr(choice, 'finish_reason') and choice.finish_reason:
                    choice_dict["finish_reason"] = choice.finish_reason
                
                chunk_dict["choices"].append(choice_dict)
        
        # usageæƒ…å ±ãŒã‚ã‚Œã°è¿½åŠ 
        if hasattr(chunk, 'usage') and chunk.usage:
            chunk_dict["usage"] = {
                "prompt_tokens": chunk.usage.prompt_tokens if hasattr(chunk.usage, 'prompt_tokens') else 0,
                "completion_tokens": chunk.usage.completion_tokens if hasattr(chunk.usage, 'completion_tokens') else 0,
                "total_tokens": chunk.usage.total_tokens if hasattr(chunk.usage, 'total_tokens') else 0
            }
        
        return chunk_dict
    
    
    def _process_response_output(self, response) -> Dict[str, Any]:
        """
        Responses APIã®éã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å¿œç­”ã‚’å‡¦ç†
        """
        return {
            "id": response.id if hasattr(response, 'id') else None,
            "object": "response",
            "output_text": response.output_text if hasattr(response, 'output_text') else "",
            "output": response.output if hasattr(response, 'output') else [],
            "model": response.model if hasattr(response, 'model') else self.default_model,
            "created_at": response.created_at if hasattr(response, 'created_at') else datetime.now().timestamp(),
            "type": "response_complete"
        }
    
    def _process_response_stream_event(self, event) -> Dict[str, Any]:
        """
        Responses APIã®ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã‚¤ãƒ™ãƒ³ãƒˆã‚’å‡¦ç†
        
        ã‚¤ãƒ™ãƒ³ãƒˆã‚¿ã‚¤ãƒ—:
        - response.output_text.delta: ãƒ†ã‚­ã‚¹ãƒˆã®ãƒ‡ãƒ«ã‚¿
        - response.completed: å¿œç­”å®Œäº†
        - error: ã‚¨ãƒ©ãƒ¼
        """
        # ã‚¤ãƒ™ãƒ³ãƒˆã‚¿ã‚¤ãƒ—ã«å¿œã˜ã¦å‡¦ç†
        event_type = getattr(event, 'type', None)
        
        if event_type == 'response.output_text.delta':
            # ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ«ã‚¿ã‚¤ãƒ™ãƒ³ãƒˆ
            return {
                "type": "text_delta",
                "content": event.delta if hasattr(event, 'delta') else "",
                "id": event.id if hasattr(event, 'id') else None
            }
        elif event_type == 'response.completed':
            # å®Œäº†ã‚¤ãƒ™ãƒ³ãƒˆ
            return {
                "type": "response_complete",
                "id": event.response_id if hasattr(event, 'response_id') else None,
                "output_text": event.output_text if hasattr(event, 'output_text') else ""
            }
        elif event_type == 'error':
            # ã‚¨ãƒ©ãƒ¼ã‚¤ãƒ™ãƒ³ãƒˆ
            return {
                "type": "error",
                "error": str(event.error) if hasattr(event, 'error') else "Unknown error"
            }
        else:
            # ãã®ä»–ã®ã‚¤ãƒ™ãƒ³ãƒˆï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
            return {
                "type": "event",
                "event_type": event_type,
                "data": str(event)
            }
    
    def _process_response(self, response) -> Dict[str, Any]:
        """éã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’å‡¦ç†"""
        response_dict = {
            "id": response.id,
            "model": response.model,
            "created": response.created,
            "object": "chat.completion",
            "choices": [],
            "usage": {
                "prompt_tokens": response.usage.prompt_tokens,
                "completion_tokens": response.usage.completion_tokens,
                "total_tokens": response.usage.total_tokens
            }
        }
        
        # choicesã‚’å‡¦ç†
        for choice in response.choices:
            choice_dict = {
                "index": choice.index,
                "message": {
                    "role": choice.message.role,
                    "content": choice.message.content
                },
                "finish_reason": choice.finish_reason
            }
            
            # ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ãŒã‚ã‚‹å ´åˆ
            if hasattr(choice.message, 'tool_calls') and choice.message.tool_calls:
                choice_dict["message"]["tool_calls"] = []
                
                for tc in choice.message.tool_calls:
                    tool_call = {
                        "id": tc.id,
                        "type": tc.type
                    }
                    
                    # Webæ¤œç´¢ãƒ„ãƒ¼ãƒ«
                    if tc.type == "web_search":
                        tool_call["web_search"] = {
                            "query": tc.web_search.query if hasattr(tc, 'web_search') else None
                        }
                    
                    # ãƒ•ã‚¡ã‚¤ãƒ«æ¤œç´¢ãƒ„ãƒ¼ãƒ«
                    elif tc.type == "file_search":
                        tool_call["file_search"] = {}
                    
                    # é–¢æ•°å‘¼ã³å‡ºã—
                    elif tc.type == "function":
                        tool_call["function"] = {
                            "name": tc.function.name,
                            "arguments": tc.function.arguments
                        }
                    
                    choice_dict["message"]["tool_calls"].append(tool_call)
            
            response_dict["choices"].append(choice_dict)
        
        return response_dict
    
    async def handle_tool_calls(
        self,
        tool_calls: List[Dict[str, Any]],
        messages: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """
        ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ã‚’å‡¦ç†
        
        Args:
            tool_calls: ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ã®ãƒªã‚¹ãƒˆ
            messages: ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å±¥æ­´ï¼ˆã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆç”¨ï¼‰
        
        Returns:
            ãƒ„ãƒ¼ãƒ«å®Ÿè¡Œçµæœã®ãƒªã‚¹ãƒˆ
        """
        tool_results = []
        
        for tool_call in tool_calls:
            tool_id = tool_call.get("id", f"tool_{datetime.now().timestamp()}")
            tool_type = tool_call.get("type")
            
            if tool_type == "web_search":
                # Webæ¤œç´¢ã‚’å®Ÿè¡Œ
                query = tool_call.get("web_search", {}).get("query", "")
                result = await self._handle_web_search(query)
                tool_results.append({
                    "tool_call_id": tool_id,
                    "role": "tool",
                    "content": result
                })
            
            elif tool_type == "file_search":
                # ãƒ•ã‚¡ã‚¤ãƒ«æ¤œç´¢ã‚’å®Ÿè¡Œ
                result = await self._handle_file_search(messages)
                tool_results.append({
                    "tool_call_id": tool_id,
                    "role": "tool",
                    "content": result
                })
            
            elif tool_type == "function":
                # é–¢æ•°å‘¼ã³å‡ºã—ã‚’å®Ÿè¡Œ
                function_name = tool_call.get("function", {}).get("name")
                arguments = tool_call.get("function", {}).get("arguments", "{}")
                
                if function_name == "web_search":
                    # Webæ¤œç´¢é–¢æ•°ã¨ã—ã¦å‡¦ç†
                    try:
                        args = json.loads(arguments)
                        query = args.get("query", "")
                        result = await self._handle_web_search(query)
                    except json.JSONDecodeError:
                        result = f"ã‚¨ãƒ©ãƒ¼: å¼•æ•°ã®ãƒ‘ãƒ¼ã‚¹ã«å¤±æ•—ã—ã¾ã—ãŸ: {arguments}"
                    
                elif function_name == "file_search":
                    # ãƒ•ã‚¡ã‚¤ãƒ«æ¤œç´¢é–¢æ•°ã¨ã—ã¦å‡¦ç†
                    result = await self._handle_file_search(messages)
                    
                else:
                    # ãã®ä»–ã®ã‚«ã‚¹ã‚¿ãƒ é–¢æ•°
                    result = await self._handle_function_call(function_name, arguments)
                
                tool_results.append({
                    "tool_call_id": tool_id,
                    "role": "tool",
                    "content": result
                })
        
        return tool_results
    
    async def _handle_web_search(self, query: str) -> str:
        """
        Webæ¤œç´¢ã‚’å‡¦ç†ï¼ˆå®Ÿéš›ã®å®Ÿè£…ã§ã¯Bing APIãªã©ã‚’ä½¿ç”¨ï¼‰
        
        Args:
            query: æ¤œç´¢ã‚¯ã‚¨ãƒª
        
        Returns:
            æ¤œç´¢çµæœ
        """
        # ãƒ­ã‚°å‡ºåŠ›ï¼šWebæ¤œç´¢ã®å®Ÿè¡Œ
        app_logger.info("="*60)
        app_logger.info("ğŸ” Webæ¤œç´¢å®Ÿè¡Œ")
        app_logger.info(f"   æ¤œç´¢ã‚¯ã‚¨ãƒª: {query}")
        
        # ã“ã“ã¯å®Ÿéš›ã®Webæ¤œç´¢APIã®å®Ÿè£…ã«ç½®ãæ›ãˆã‚‹
        # ä¾‹: Bing Search APIã€Google Custom Search API ãªã©
        
        # ãƒ‡ãƒ¢ç”¨ã®ä»®ã®çµæœ
        max_results = self.tools_config.get_setting("web_search_max_results", 5)
        
        app_logger.info(f"   æœ€å¤§çµæœæ•°: {max_results}")
        app_logger.info("="*60)
        
        # æ¤œç´¢çµæœã«ã‚½ãƒ¼ã‚¹æƒ…å ±ã‚’å«ã‚ã‚‹
        result = f"\nğŸ” **Webæ¤œç´¢çµæœ**\n\n"
        result += f"**æ¤œç´¢ã‚¯ã‚¨ãƒª:** `{query}`\n"
        result += f"**çµæœæ•°:** æœ€å¤§{max_results}ä»¶\n\n"
        
        # ãƒ‡ãƒ¢ç”¨ã®ä»®ã®çµæœã¨ã‚½ãƒ¼ã‚¹
        result += "**æ¤œç´¢çµæœ:**\n"
        result += f"1. ğŸ“Œ [é–¢é€£ã‚µã‚¤ãƒˆ1] {query}ã«é–¢ã™ã‚‹æœ€æ–°æƒ…å ±\n"
        result += f"   - ã‚½ãƒ¼ã‚¹: https://example1.com\n\n"
        result += f"2. ğŸ“Œ [é–¢é€£ã‚µã‚¤ãƒˆ2] {query}ã®è©³ç´°è§£èª¬\n"
        result += f"   - ã‚½ãƒ¼ã‚¹: https://example2.com\n\n"
        
        result += "âš ï¸ æ³¨: ã“ã‚Œã¯ãƒ‡ãƒ¢çµæœã§ã™ã€‚å®Ÿéš›ã®Webæ¤œç´¢APIã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚"
        
        return result
    
    async def _handle_file_search(self, messages: List[Dict[str, Any]]) -> str:
        """
        ãƒ•ã‚¡ã‚¤ãƒ«æ¤œç´¢ã‚’å‡¦ç†ï¼ˆãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢å‚ç…§ãƒ­ã‚°ä»˜ãï¼‰
        
        Args:
            messages: ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å±¥æ­´ï¼ˆã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆç”¨ï¼‰
        
        Returns:
            æ¤œç´¢çµæœ
        """
        # ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã‚’å–å¾—
        active_stores = vector_store_handler.get_active_vector_stores()
        
        # ãƒ­ã‚°å‡ºåŠ›ï¼šã©ã®å±¤ã®ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ãŒå‚ç…§ã•ã‚Œã‚‹ã‹
        app_logger.info("="*60)
        app_logger.info("ğŸ“š ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢å‚ç…§é–‹å§‹")
        app_logger.info("-"*60)
        
        referenced_layers = []
        vs_info = []
        
        # 1å±¤ç›®: ä¼šç¤¾å…¨ä½“ï¼ˆCompanyï¼‰
        if "company" in active_stores:
            vs_id = active_stores["company"]
            app_logger.info(f"ğŸ¢ ã€1å±¤ç›®ã€‘ä¼šç¤¾å…±æœ‰ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢")
            app_logger.info(f"   â””â”€ ID: {vs_id}")
            referenced_layers.append("1å±¤ç›®:ä¼šç¤¾å…±æœ‰")
            vs_info.append({"layer": "ä¼šç¤¾å…±æœ‰", "id": vs_id})
        
        # 2å±¤ç›®: å€‹äººãƒ¦ãƒ¼ã‚¶ãƒ¼ï¼ˆPersonalï¼‰
        if "personal" in active_stores:
            vs_id = active_stores["personal"]
            app_logger.info(f"ğŸ‘¤ ã€2å±¤ç›®ã€‘å€‹äººç”¨ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢")
            app_logger.info(f"   â””â”€ ID: {vs_id}")
            referenced_layers.append("2å±¤ç›®:å€‹äººç”¨")
            vs_info.append({"layer": "å€‹äººç”¨", "id": vs_id})
        
        # 3å±¤ç›®: ãƒãƒ£ãƒƒãƒˆå˜ä½ï¼ˆSessionï¼‰
        if "session" in active_stores:
            vs_id = active_stores["session"]
            app_logger.info(f"ğŸ’¬ ã€3å±¤ç›®ã€‘ã‚»ãƒƒã‚·ãƒ§ãƒ³ç”¨ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢")
            app_logger.info(f"   â””â”€ ID: {vs_id}")
            referenced_layers.append("3å±¤ç›®:ã‚»ãƒƒã‚·ãƒ§ãƒ³ç”¨")
            vs_info.append({"layer": "ã‚»ãƒƒã‚·ãƒ§ãƒ³ç”¨", "id": vs_id})
        
        if not active_stores:
            app_logger.warning("âš ï¸ ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ãŒã‚ã‚Šã¾ã›ã‚“")
            app_logger.info("="*60)
            return "æ¤œç´¢å¯¾è±¡ã®ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚"
        
        app_logger.info("-"*60)
        app_logger.info(f"âœ… å‚ç…§ã•ã‚ŒãŸãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢: {', '.join(referenced_layers)}")
        app_logger.info("="*60)
        
        # å®Ÿéš›ã®ãƒ•ã‚¡ã‚¤ãƒ«æ¤œç´¢å®Ÿè£…
        file_ids = self.tools_config.get_search_file_ids()
        max_chunks = self.tools_config.get_setting("file_search_max_chunks", 20)
        
        # æ¤œç´¢çµæœã«ã‚½ãƒ¼ã‚¹æƒ…å ±ã‚’å«ã‚ã‚‹
        result = f"\nğŸ“š **ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢æ¤œç´¢çµæœ**\n\n"
        result += f"ğŸ” **å‚ç…§ã•ã‚ŒãŸãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢:**\n"
        
        for info in vs_info:
            result += f"  - {info['layer']}: `{info['id']}`\n"
        
        result += f"\nğŸ“Š **æ¤œç´¢ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:**\n"
        result += f"  - æœ€å¤§ãƒãƒ£ãƒ³ã‚¯æ•°: {max_chunks}\n"
        
        if file_ids:
            result += f"  - ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(file_ids)}\n"
            result += f"  - ãƒ•ã‚¡ã‚¤ãƒ«IDï¼ˆä¸€éƒ¨ï¼‰: {', '.join(file_ids[:3])}...\n"
        
        # ãƒ‡ãƒ¢çµæœã®å ´åˆã®æ³¨è¨˜
        result += f"\nâš ï¸ æ³¨: å®Ÿéš›ã®ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢æ¤œç´¢çµæœãŒã“ã“ã«è¡¨ç¤ºã•ã‚Œã¾ã™ã€‚"
        
        return result
    
    async def _handle_function_call(self, function_name: str, arguments: str) -> str:
        """
        ã‚«ã‚¹ã‚¿ãƒ é–¢æ•°å‘¼ã³å‡ºã—ã‚’å‡¦ç†
        
        Args:
            function_name: é–¢æ•°å
            arguments: é–¢æ•°ã®å¼•æ•°ï¼ˆJSONæ–‡å­—åˆ—ï¼‰
        
        Returns:
            é–¢æ•°ã®å®Ÿè¡Œçµæœ
        """
        try:
            args = json.loads(arguments)
        except json.JSONDecodeError:
            return f"ã‚¨ãƒ©ãƒ¼: å¼•æ•°ã®ãƒ‘ãƒ¼ã‚¹ã«å¤±æ•—ã—ã¾ã—ãŸ: {arguments}"
        
        # ã‚«ã‚¹ã‚¿ãƒ é–¢æ•°ã®å®Ÿè£…ã‚’ã“ã“ã«è¿½åŠ 
        # ä¾‹: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¤œç´¢ã€å¤–éƒ¨APIå‘¼ã³å‡ºã—ãªã©
        
        return f"é–¢æ•° {function_name} ã‚’å®Ÿè¡Œã—ã¾ã—ãŸï¼ˆå¼•æ•°: {args}ï¼‰"
    
    def format_messages_for_api(
        self,
        messages: List[Dict[str, Any]],
        system_prompt: str = None
    ) -> List[Dict[str, str]]:
        """
        ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å±¥æ­´ã‚’Chat Completions APIç”¨ã«ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
        
        Args:
            messages: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰å–å¾—ã—ãŸãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
            system_prompt: ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        
        Returns:
            APIç”¨ã«ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã•ã‚ŒãŸãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒªã‚¹ãƒˆ
        """
        formatted = []
        
        # ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’è¿½åŠ 
        if system_prompt:
            formatted.append({
                "role": "system",
                "content": system_prompt
            })
        
        # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å±¥æ­´ã‚’å¤‰æ›
        for msg in messages:
            role = msg.get("role", "user")
            content = msg.get("content", "")
            
            message = {
                "role": role,
                "content": content
            }
            
            # ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ãŒã‚ã‚‹å ´åˆ
            if "tool_calls" in msg:
                message["tool_calls"] = msg["tool_calls"]
            
            # ãƒ„ãƒ¼ãƒ«çµæœã®å ´åˆ
            if role == "tool" and "tool_call_id" in msg:
                message["tool_call_id"] = msg["tool_call_id"]
            
            formatted.append(message)
        
        return formatted
    
    def calculate_token_estimate(self, text: str) -> int:
        """
        ãƒ†ã‚­ã‚¹ãƒˆã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’æ¨å®š
        
        Args:
            text: ãƒ†ã‚­ã‚¹ãƒˆ
        
        Returns:
            æ¨å®šãƒˆãƒ¼ã‚¯ãƒ³æ•°
        """
        # ç°¡æ˜“çš„ãªæ¨å®šï¼ˆå®Ÿéš›ã¯tiktokenã‚’ä½¿ã†ã¹ãï¼‰
        # æ—¥æœ¬èª: 1æ–‡å­— â‰ˆ 2-3ãƒˆãƒ¼ã‚¯ãƒ³
        # è‹±èª: 1å˜èª â‰ˆ 1-1.5ãƒˆãƒ¼ã‚¯ãƒ³
        return len(text) // 3
    
    async def generate_title(self, messages: List[Dict[str, str]]) -> str:
        """
        ä¼šè©±ã‹ã‚‰ã‚¿ã‚¤ãƒˆãƒ«ã‚’è‡ªå‹•ç”Ÿæˆ
        
        Args:
            messages: ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å±¥æ­´
        
        Returns:
            ç”Ÿæˆã•ã‚ŒãŸã‚¿ã‚¤ãƒˆãƒ«
        """
        if not self.async_client:
            return f"Chat - {datetime.now().strftime('%Y-%m-%d %H:%M')}"
        
        try:
            # ã‚¿ã‚¤ãƒˆãƒ«ç”Ÿæˆç”¨ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
            title_prompt = [
                {"role": "system", "content": "ä»¥ä¸‹ã®ä¼šè©±ã‹ã‚‰ã€çŸ­ãç°¡æ½”ãªã‚¿ã‚¤ãƒˆãƒ«ã‚’æ—¥æœ¬èªã§ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚20æ–‡å­—ä»¥å†…ã§ã€‚"},
                *messages[:3]  # æœ€åˆã®3ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ã¿ä½¿ç”¨
            ]
            
            # ãƒ„ãƒ¼ãƒ«ã‚’ä½¿ã‚ãšã«ã‚¿ã‚¤ãƒˆãƒ«ç”Ÿæˆ
            # Responses APIã‚’ä¼˜å…ˆçš„ã«ä½¿ç”¨ã€å¤±æ•—æ™‚ã¯Chat Completions APIã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            try:
                # Responses APIã‚’è©¦ã™
                response = await self.async_client.responses.create(
                    model="gpt-4o-mini",
                    input="ä»¥ä¸‹ã®ä¼šè©±ã‹ã‚‰ã€çŸ­ãç°¡æ½”ãªã‚¿ã‚¤ãƒˆãƒ«ã‚’æ—¥æœ¬èªã§ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚20æ–‡å­—ä»¥å†…ã§ã€‚",
                    instructions="\n".join([f"{m['role']}: {m['content']}" for m in messages[:3]]),
                    temperature=0.5,
                    max_tokens=30,
                    stream=False
                )
                title = response.output_text.strip() if hasattr(response, 'output_text') else response.output.strip()
            except (AttributeError, Exception) as e:
                # Chat Completions APIã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                response = await self.async_client.chat.completions.create(
                    model="gpt-4o-mini",
                    messages=title_prompt,
                    temperature=0.5,
                    max_tokens=30,
                    stream=False
                )
                title = response.choices[0].message.content.strip()
            # ã‚¿ã‚¤ãƒˆãƒ«ãŒé•·ã™ãã‚‹å ´åˆã¯åˆ‡ã‚Šè©°ã‚
            if len(title) > 30:
                title = title[:27] + "..."
            
            return title
        
        except Exception as e:
            print(f"Error generating title: {e}")
            return f"Chat - {datetime.now().strftime('%Y-%m-%d %H:%M')}"
    
    def format_token_usage(self, usage: Dict[str, int]) -> str:
        """
        ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡ã‚’ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
        
        Args:
            usage: ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡
        
        Returns:
            ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã•ã‚ŒãŸæ–‡å­—åˆ—
        """
        if not usage:
            return ""
        
        prompt = usage.get("prompt_tokens", 0)
        completion = usage.get("completion_tokens", 0)
        total = usage.get("total_tokens", 0)
        
        # æ¦‚ç®—ã‚³ã‚¹ãƒˆè¨ˆç®—ï¼ˆGPT-4o-miniã®æ–™é‡‘: $0.15/1M input, $0.6/1M outputï¼‰
        input_cost = prompt * 0.00000015
        output_cost = completion * 0.0000006
        total_cost = input_cost + output_cost
        
        return f"ğŸ“Š ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡: å…¥åŠ› {prompt} + å‡ºåŠ› {completion} = åˆè¨ˆ {total} (ç´„${total_cost:.4f})"


# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
responses_handler = ResponsesAPIHandler()
